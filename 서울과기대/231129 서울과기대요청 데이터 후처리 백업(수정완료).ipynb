{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀파일이 'phenotype mappingv3.xlsx', '1_검증.csv' 이렇게 2개가 있어\n",
    "#  'phenotype mappingv3.xlsx' 파일은 UMLSID, Group, GroupID 이렇게 3개의 컬럼이 존재하고\n",
    "# '1_검증.csv' 파일은 Phenotype, Network_score, z-score, P-value, UMLSID 이렇게 5개의 컬럼이 존재해\n",
    "# 여기서 '1_검증.csv' 파일 내에 있는 UMLSID의 데이터와  'phenotype mappingv3.xlsx' 파일 내에 있는 UMLSID의 데이터가 일치하면 Group, GroupID 값을 가져와서 똑같이 써주고, 아니면 None 값을 써주고 싶어\n",
    "# 그래서 최종적으로 '1_검증.csv' 파일내에 Phenotype, Network_score, z-score, P-value, UMLSID, Group, GroupID 이렇게 7개의 컬럼이 존재하게 만들고 싶어\n",
    "# 이렇게 코드 짜줘\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# phenotype_mapping_file='/data/home/sss2061/식약처보고서/서울과기대/phenotype mappingv3.xlsx'\n",
    "\n",
    "# '1_검증.csv' 파일을 읽어옵니다.\n",
    "\n",
    "number=2 #수정 필요\n",
    "validation_df = pd.read_csv('result/{}차/{}_검증.csv'.format(number,number)) #수정 필요\n",
    "\n",
    "# 'phenotype mappingv3.xlsx' 파일을 읽어옵니다.\n",
    "mapping_df = pd.read_excel('phenotype mappingv3.xlsx', engine='openpyxl')\n",
    "\n",
    "# '1_검증.csv' 파일 내의 UMLSID와 'phenotype mappingv3.xlsx' 파일 내의 UMLSID를 기반으로 병합합니다.\n",
    "merged_df = validation_df.merge(mapping_df, on='UMLSID', how='left')\n",
    "\n",
    "# Group 및 GroupID가 없는 경우 None으로 채웁니다.\n",
    "merged_df['Group'].fillna('None', inplace=True)\n",
    "merged_df['GroupID'].fillna('None', inplace=True)\n",
    "\n",
    "# 'Network_score' 컬럼의 데이터를 추출합니다.\n",
    "network_scores = merged_df['Network_score']\n",
    "\n",
    "# Min-Max Scaling을 0부터 1로 설정하여 적용합니다.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_scores = scaler.fit_transform(network_scores.values.reshape(-1, 1))  # 2D 배열로 변환 후 스케일링\n",
    "\n",
    "# 스케일링된 값을 'scaling_score' 컬럼에 추가합니다.\n",
    "merged_df['scaling_score'] = scaled_scores\n",
    "\n",
    "# 'P-value'의 값을 조정합니다.\n",
    "def adjust_pvalue(value):\n",
    "    if value == 999 or value == 1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return (value+1) / 1000\n",
    "\n",
    "merged_df['P-value'] = merged_df['P-value'].apply(adjust_pvalue)\n",
    "\n",
    "cols_to_drop = [col for col in merged_df.columns if col.startswith(\"Unnamed\")]\n",
    "merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "hpo_df=pd.read_csv(\"Data/[original] HPO phenotype data.csv\")\n",
    "\n",
    "merged_df=pd.merge(merged_df, hpo_df, left_on='Phenotype', right_on='phenotype name', how='left')\n",
    "\n",
    "final_df=merged_df[['Phenotype','Network_score','z-score','P-value','UMLSID','Group','GroupID','scaling_score', 'hpo_id']]\n",
    "\n",
    "# 수정된 DataFrame을 새로운 엑셀 파일로 저장합니다.\n",
    "final_df.to_csv('result/{}차/{}_검증_with_scaling.csv'.format(number,number), index=False)  # 저장할 파일명을 지정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 백업본\n",
    "\n",
    "# 엑셀파일이 'phenotype mappingv3.xlsx', '1_검증.csv' 이렇게 2개가 있어\n",
    "#  'phenotype mappingv3.xlsx' 파일은 UMLSID, Group, GroupID 이렇게 3개의 컬럼이 존재하고\n",
    "# '1_검증.csv' 파일은 Phenotype, Network_score, z-score, P-value, UMLSID 이렇게 5개의 컬럼이 존재해\n",
    "# 여기서 '1_검증.csv' 파일 내에 있는 UMLSID의 데이터와  'phenotype mappingv3.xlsx' 파일 내에 있는 UMLSID의 데이터가 일치하면 Group, GroupID 값을 가져와서 똑같이 써주고, 아니면 None 값을 써주고 싶어\n",
    "# 그래서 최종적으로 '1_검증.csv' 파일내에 Phenotype, Network_score, z-score, P-value, UMLSID, Group, GroupID 이렇게 7개의 컬럼이 존재하게 만들고 싶어\n",
    "# 이렇게 코드 짜줘\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "\n",
    "# phenotype_mapping_file='/data/home/sss2061/식약처보고서/서울과기대/phenotype mappingv3.xlsx'\n",
    "\n",
    "# '1_검증.csv' 파일을 읽어옵니다.\n",
    "\n",
    "number=2\n",
    "validation_df = pd.read_csv('result/{}차/{}_검증.csv'.format(number,number))\n",
    "\n",
    "# 'phenotype mappingv3.xlsx' 파일을 읽어옵니다.\n",
    "mapping_df = pd.read_excel('phenotype mappingv3.xlsx', engine='openpyxl')\n",
    "\n",
    "# '1_검증.csv' 파일 내의 UMLSID와 'phenotype mappingv3.xlsx' 파일 내의 UMLSID를 기반으로 병합합니다.\n",
    "merged_df = validation_df.merge(mapping_df, on='UMLSID', how='left')\n",
    "\n",
    "# Group 및 GroupID가 없는 경우 None으로 채웁니다.\n",
    "merged_df['Group'].fillna('None', inplace=True)\n",
    "merged_df['GroupID'].fillna('None', inplace=True)\n",
    "\n",
    "# 결과를 'merged_1_검증.csv' 파일로 저장합니다.\n",
    "merged_df.to_csv('merged_14_검증.csv', index=False)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 'merged_1_검증.xlsx' 파일을 읽어옵니다.\n",
    "df = pd.read_csv('merged_14_검증.csv')  # 파일 경로를 확인해주세요.\n",
    "\n",
    "# 'Network_score' 컬럼의 데이터를 추출합니다.\n",
    "network_scores = df['Network_score']\n",
    "\n",
    "# Min-Max Scaling을 0부터 1로 설정하여 적용합니다.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_scores = scaler.fit_transform(network_scores.values.reshape(-1, 1))  # 2D 배열로 변환 후 스케일링\n",
    "\n",
    "# 스케일링된 값을 'scaling_score' 컬럼에 추가합니다.\n",
    "df['scaling_score'] = scaled_scores\n",
    "\n",
    "# 'P-value'의 값을 조정합니다.\n",
    "def adjust_pvalue(value):\n",
    "    if value == 999 or value == 1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return (value+1) / 1000\n",
    "\n",
    "df['P-value'] = df['P-value'].apply(adjust_pvalue)\n",
    "\n",
    "# \"Unnamed\"로 시작하는 컬럼을 찾아 삭제합니다.\n",
    "cols_to_drop = [col for col in df.columns if col.startswith(\"Unnamed\")]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# 수정된 DataFrame을 새로운 엑셀 파일로 저장합니다.\n",
    "df.to_csv('merged_14_검증_with_scaling.csv', index=False)  # 저장할 파일명을 지정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number=1\n",
    "# text=[\"adenine\", \"canavanin\", \"malate\", \"isoleucine\", \"phenylalanine\", \"tyrosine\", \"isocitrate\", \"glutamine\", \"tryptophan\",\"L-malate\", \n",
    "#       \"theogallin\", \"trans-aconitate\", \"cis-aconitate\", \"threonate\", \"benzoic acid\", \"choline\", \"citric acid\", \"coumarin\", \"phloroglucinol\", \"gallic acid\", \n",
    "#       \"citrullin\", \"pipecolic acid\", \"vitamin B\", \"pantothenic acid\", \"DL-valine\", \"vanillin\", \"theophylline\", \"caffeine\", \"protocatechuic aldehyde\", \"adenosine\", \n",
    "#       \"EGCG\", \"gallocatechin\", \"N-acetylglutamate\", \"loliolide\", \"procyanidin B2\", \"3H-proline\", \"dihydromyricetin\", \"beta-alanine betaine\", \"epicatechin gallate (ECG\", \"5'-methylthioadenosine\", \n",
    "#       \"epiafzelechin\", \"ferulic acid\", \"p-coumaric acid\", \"caffeic acid\", \"chlorogenic acid\", \"quercetin\", \"alpha-isopropylmalate\", \"neochlorogenic acid\", \"rutin\", \"hyperoside\", \n",
    "#       \"4-p-coumaroylquinic acid\", \"astragalin\", \"quercetin 7-O-glucoside\", \"p-coumaroylquinic acid\", \"procyanidin B1\"]\n",
    "\n",
    "# number=2\n",
    "# text=[\"gamma-aminobutyric acid (GABA\", \"canavanin\", \"sucrose\", \"folate\", \"guanosine\", \"vanillic acid\", \"ononin\", \n",
    "#       \"formononetin\", \"calycosin\", \"sissotrin\", \"afrormosin\", \"pratensein\"]\n",
    "\n",
    "# number=3\n",
    "# text=[\"protocatechuic acid\", \"gamma-aminobutyric acid (GABA\", \"choline\", \"citric acid\", \"malate\", \"asparagine\", \n",
    "#       \"quinic acid\", \"maltol\", \"nodakenetin\", \"nodakenin\", \"columbianetin\", \"decursin\", \"ferulic acid\", \"caffeic acid\",\n",
    "#       \"aegelinol\", \"cis-chlorogenic acid\", \"chlorogenic acid\", \"esculetin\"]\n",
    "\n",
    "# number=4\n",
    "# text=[\"piperine\", \"piperanine\", \"piperlonguminine\", \"retrofractamide B\", \"dehydropipernonaline\", \"Methyl piperate\", \"pipernonaline\", \"Retrofractamide A\", \"retrofractamide C\"]\n",
    "\n",
    "# number=5\n",
    "# text=[\"apocynin\", \"resacetophenone\", \"4-hydroxyacetophenone\", \"2,5-dihydroxyacetophenone\", \"physcion\"]\n",
    "\n",
    "# number=6\n",
    "# text=[\"protocatechuic acid\", \"p-hydroxybenzaldehyde\", \"choline\", \"gluconate\", \"myo-inositol\", \"thymine\", \"uracil\", \"vanillin\", \"vanillic acid\", \"protocatechuic aldehyde\", \"trans-aconitate\", \"ferulic acid\", \"caffeic acid\", \"chlorogenic acid\", \"linoleic acid\", \"neochlorogenic acid\", \"xanthatin\", \"cynarin\", \"cynarine\"]\n",
    "\n",
    "# number=7\n",
    "# text=[\"atractylenolide III\", \"vanillic acid\", \"esculetin\", \"elemicin\", \"adenosine\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951418c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#결과 후처리\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "import pickle\n",
    "\n",
    "number=7\n",
    "\n",
    "with open(\"result/{}차/output_dict.pickle\".format(number),\"rb\") as fi:\n",
    "    output_dict = pickle.load(fi)\n",
    "    \n",
    "# validation_df = pd.read_csv('result/{}차/{}_검증.csv'.format(number,number))\n",
    "inferred_phenotype2 = pd.read_csv('result/{}차/{}_network_score.csv'.format(number,number))\n",
    "\n",
    "phenotype_rwr_score_dict = pd.Series(inferred_phenotype2.Network_score.values, index=inferred_phenotype2.Phenotype).to_dict()\n",
    "\n",
    "temp=list(phenotype_rwr_score_dict.keys())\n",
    "for i in temp:\n",
    "    if np.isnan(phenotype_rwr_score_dict[i])==True:\n",
    "        phenotype_rwr_score_dict.pop(i)\n",
    "\n",
    "Phenotype_score_Phenotype=list(phenotype_rwr_score_dict.keys())\n",
    "Phenotype_score_Score=list(phenotype_rwr_score_dict.values())\n",
    "\n",
    "temp2=list(output_dict.keys())\n",
    "for i in temp2:\n",
    "    if np.isnan(output_dict[i][0])==True:\n",
    "        output_dict.pop(i)\n",
    "        \n",
    "P_value_Phenotype=list(output_dict.keys())\n",
    "P_value_Score=list(output_dict.values())\n",
    "\n",
    "count=0\n",
    "z_score_list=[]\n",
    "outlier_list=[] #제거가능\n",
    "P_value_list=[]\n",
    "rank_number=0\n",
    "result_phenotype=[]\n",
    "result_phenotype_score=[]\n",
    "\n",
    "for i in range(len(P_value_Score)):\n",
    "    Numberlist=P_value_Score[i].copy()\n",
    "    tmp1=P_value_Phenotype[i]\n",
    "    tmp2=Phenotype_score_Phenotype.index(tmp1)\n",
    "    tmp3=Phenotype_score_Score[tmp2]\n",
    "    Numberlist.append(tmp3)\n",
    "    result_phenotype.append(tmp1)\n",
    "    result_phenotype_score.append(tmp3)\n",
    "    \n",
    "    # 평균 계산\n",
    "    mean = calculate_mean(Numberlist)\n",
    "    # 표준편차 계산\n",
    "    std_deviation = calculate_standard_deviation(Numberlist)\n",
    "\n",
    "    z_score=(Phenotype_score_Score[i]-mean)/std_deviation\n",
    "    z_score_list.append(z_score)\n",
    "    if z_score >=-2.58 and z_score<=2.58: #제거가능\n",
    "        outlier=1\n",
    "        count=count+1\n",
    "    else:\n",
    "        outlier=0 \n",
    "    outlier_list.append(outlier) #제거가능\n",
    "\n",
    "    Numberlist.sort(reverse=True)\n",
    "\n",
    "    rank=Numberlist.index(tmp3) #순위찾기\n",
    "    if rank<5:\n",
    "        rank_number=rank_number+1\n",
    "    P_value_list.append(rank)\n",
    "    print(\"phe\",tmp1)\n",
    "    print(\"z\",z_score)\n",
    "    print(\"p\",rank)\n",
    "\n",
    "# postgresql DB 연동\n",
    "config = configparser.ConfigParser()\n",
    "config.read('db_config.ini')\n",
    "conn=psycopg2.connect(host=\"168.131.30.66\", dbname=\"coconut\",user=\"dbuser\",password=\"jnudl1\") #데이터이름변경\n",
    "conn_cur = conn.cursor()\n",
    "\n",
    "#------------------------\n",
    "\n",
    "Phenotype_score = pd.DataFrame({\n",
    "    'Phenotype': result_phenotype,\n",
    "    'Network_score': result_phenotype_score,\n",
    "    'z-score': z_score_list,\n",
    "    'P-value': P_value_list,\n",
    "})\n",
    "\n",
    "print(Phenotype_score)\n",
    "\n",
    "Phenotype_score=Phenotype_score.sort_values(by='P-value',ascending=True)\n",
    "\n",
    "#     # Compound ID에 해당하는 inferred phenotype.csv 생성\n",
    "# if not os.path.exists('compound 결과'):\n",
    "#     os.makedirs('compound 결과')\n",
    "Phenotype_score.to_csv('result/{}차/{}_검증.csv'.format(number,number),index=False, encoding='UTF-8')\n",
    "    \n",
    "end = time.time()\n",
    "sec = (end - start)\n",
    "result = datetime.timedelta(seconds=sec)\n",
    "print(\"걸린 시간 : \",result)\n",
    "print(f\"걸린 시간 : {end - start:.5f} sec\")\n",
    "\n",
    "# 엑셀파일이 'phenotype mappingv3.xlsx', '1_검증.csv' 이렇게 2개가 있어\n",
    "#  'phenotype mappingv3.xlsx' 파일은 UMLSID, Group, GroupID 이렇게 3개의 컬럼이 존재하고\n",
    "# '1_검증.csv' 파일은 Phenotype, Network_score, z-score, P-value, UMLSID 이렇게 5개의 컬럼이 존재해\n",
    "# 여기서 '1_검증.csv' 파일 내에 있는 UMLSID의 데이터와  'phenotype mappingv3.xlsx' 파일 내에 있는 UMLSID의 데이터가 일치하면 Group, GroupID 값을 가져와서 똑같이 써주고, 아니면 None 값을 써주고 싶어\n",
    "# 그래서 최종적으로 '1_검증.csv' 파일내에 Phenotype, Network_score, z-score, P-value, UMLSID, Group, GroupID 이렇게 7개의 컬럼이 존재하게 만들고 싶어\n",
    "# 이렇게 코드 짜줘\n",
    "\n",
    "\n",
    "# 'phenotype mappingv3.xlsx' 파일을 읽어옵니다.\n",
    "mapping_df = pd.read_excel('phenotype mappingv3.xlsx', engine='openpyxl')\n",
    "\n",
    "# '1_검증.csv' 파일 내의 UMLSID와 'phenotype mappingv3.xlsx' 파일 내의 UMLSID를 기반으로 병합합니다.\n",
    "merged_df = Phenotype_score.merge(mapping_df, on='UMLSID', how='left')\n",
    "\n",
    "# Group 및 GroupID가 없는 경우 None으로 채웁니다.\n",
    "merged_df['Group'].fillna('None', inplace=True)\n",
    "merged_df['GroupID'].fillna('None', inplace=True)\n",
    "\n",
    "# 'Network_score' 컬럼의 데이터를 추출합니다.\n",
    "network_scores = merged_df['Network_score']\n",
    "\n",
    "# Min-Max Scaling을 0부터 1로 설정하여 적용합니다.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_scores = scaler.fit_transform(network_scores.values.reshape(-1, 1))  # 2D 배열로 변환 후 스케일링\n",
    "\n",
    "# 스케일링된 값을 'scaling_score' 컬럼에 추가합니다.\n",
    "merged_df['scaling_score'] = scaled_scores\n",
    "\n",
    "# 'P-value'의 값을 조정합니다.\n",
    "def adjust_pvalue(value):\n",
    "    if value == 999 or value == 1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return (value+1) / 1000\n",
    "\n",
    "merged_df['P-value'] = merged_df['P-value'].apply(adjust_pvalue)\n",
    "\n",
    "cols_to_drop = [col for col in merged_df.columns if col.startswith(\"Unnamed\")]\n",
    "merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "hpo_df=pd.read_csv(\"Data/[original] HPO phenotype data.csv\")\n",
    "\n",
    "merged_df=pd.merge(merged_df, hpo_df, left_on='Phenotype', right_on='phenotype name', how='left')\n",
    "\n",
    "final_df=merged_df[['Phenotype','Network_score','z-score','P-value','UMLSID','Group','GroupID','scaling_score', 'hpo_id']]\n",
    "\n",
    "# 수정된 DataFrame을 새로운 엑셀 파일로 저장합니다.\n",
    "final_df.to_csv('result/{}차/{}_검증_with_scaling.csv'.format(number,number), index=False)  # 저장할 파일명을 지정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b79526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file  은 9개의 컬럼이 존재해 [Phenotype,Network_score,z-score,P-value,UMLSID,Group,GroupID,scaling_score,hpo_id]\n",
    "# file_2는 4개의 컬럼이 존재해 [umlsid,phenotype,hpoid,cr] \n",
    "# 이런 상황에서 file의 UMLSID 컬럼을 지워줘\n",
    "# 그 이후에, file_2의 umlsid 컬럼을 file에 새로운 컬럼으로 추가할거야\n",
    "# file과 Phenotype 컬럼의 값과 file_2의 phenotype 컬럼의 값들을 비교해서 일치하면 umlsid 컬럼의 값을 가지도록 해서\n",
    "# 최종적으로 file3 은 9개의 컬럼을 가지게 해줘   [Phenotype,Network_score,z-score,P-value,UMLSID,Group,GroupID,scaling_score,hpo_id]\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "import pickle\n",
    "\n",
    "# 1. umlsid\n",
    "# 2. ppi network\n",
    "\n",
    "number=7\n",
    "\n",
    "file = pd.read_csv('result/{}차/{}_검증_with_scaling.csv'.format(number,number))\n",
    "\n",
    "file_2= pd.read_csv(\"Data/수정_Id2.csv\")\n",
    "\n",
    "# 'UMLSID' 컬럼을 file에서 제거합니다.\n",
    "file.drop(columns=['UMLSID'], inplace=True)\n",
    "\n",
    "# file과 file_2를 'Phenotype'과 'phenotype' 컬럼을 기준으로 결합합니다.\n",
    "# 이때 file_2의 'umlsid' 컬럼만 선택하여 추가합니다.\n",
    "file_3 = pd.merge(file, file_2[['umlsid', 'phenotype']], left_on='Phenotype', right_on='phenotype', how='left')\n",
    "\n",
    "# 중복된 'phenotype' 컬럼을 제거합니다.\n",
    "file_3.drop(columns=['phenotype'], inplace=True)\n",
    "\n",
    "# 최종 파일을 저장합니다.\n",
    "file_3.to_csv('result/{}차/{}_검증_with_scaling.csv'.format(number,number), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba949dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HPO gene_phenotype 데이터와 PPI network 내에서 entrez_list들의 교집합을 phenotype / entrez_id 형식으로 변환\n",
    "\n",
    "# herb 최종\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def calculate_mean(numbers):\n",
    "    return sum(numbers) / len(numbers)\n",
    "\n",
    "def calculate_standard_deviation(numbers):\n",
    "    return statistics.stdev(numbers)\n",
    "\n",
    "def combine_lists(full_list):\n",
    "    combined = defaultdict(list)\n",
    "    \n",
    "    # 모든 리스트에 대한 처리\n",
    "    for sublist in full_list:\n",
    "        for item in sublist:\n",
    "            entrez_id, rwr_score = item\n",
    "            combined[entrez_id].append(rwr_score)\n",
    "    \n",
    "    # 조화평균 및 결과 리스트 만들기\n",
    "    result = []\n",
    "    for entrez_id, scores in combined.items():\n",
    "        if len(scores) == 1:\n",
    "            result.append([entrez_id, scores[0]])  # 값이 하나면 그대로 추가\n",
    "        else:\n",
    "            # 조화평균 계산\n",
    "            harmonic_mean_score = harmonic_mean(scores)\n",
    "            result.append([entrez_id, harmonic_mean_score])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compound_target(compoundid):\n",
    "\n",
    "    conn_cur.execute(f\"with tmp1 as (select cg.geneid from compound_gene cg where (cg.compid = '{compoundid}' and cg.relationsign in ('activate','inhibit')))\" \n",
    "                    f\"select g.entrezid from gene g,tmp1 where g.geneid=tmp1.geneid;\")\n",
    "    compound_gene1=conn_cur.fetchall()\n",
    "\n",
    "    compound_gene=set(compound_gene1)\n",
    "    compound_gene=list(compound_gene)\n",
    "    \n",
    "    # Compound-target gene List 생성\n",
    "    target_gene_list = [] \n",
    "    for i in range(len(compound_gene)):\n",
    "        interaction_gene = compound_gene[i][0]\n",
    "        if interaction_gene in entrez_list:\n",
    "            target_gene_list.append(interaction_gene)        \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # restart node에 적용할 target gene List 생성\n",
    "    target_node = [] \n",
    "    for entrez in target_gene_list:\n",
    "        node_ = node_index.get(entrez) \n",
    "        target_node.append(node_)\n",
    "\n",
    "    conn_cur.execute(f\"with tmp2 as (select cg.geneid from compound_gene cg where cg.compid = '{compoundid}' and relationsign='associate')\" \n",
    "                    f\"select g.entrezid from gene g,tmp2 where g.geneid=tmp2.geneid;\")\n",
    "\n",
    "    compound_gene2=conn_cur.fetchall()\n",
    "\n",
    "    compound_gene=set(compound_gene2)\n",
    "    compound_gene=list(compound_gene)\n",
    "\n",
    "    indirected_target_gene_list = [] \n",
    "    for i in range(len(compound_gene)):\n",
    "        interaction_gene = compound_gene[i][0]\n",
    "        if interaction_gene in entrez_list:\n",
    "            indirected_target_gene_list.append(interaction_gene)        \n",
    "        else:\n",
    "            continue      \n",
    "\n",
    "    num1=len(target_gene_list) #target gene list 갯수\n",
    "    directed_target_gene_list.append(num1)\n",
    "    num2=len(indirected_target_gene_list) #indirected target gene list 갯수 \n",
    "    indirected_target_gene_list_count.append(num2)\n",
    "    \n",
    "    #indirect nodes 정의\n",
    "    indirect_nodes= []\n",
    "    for entrez in indirected_target_gene_list:\n",
    "        node_ = node_index.get(entrez) \n",
    "        indirect_nodes.append(node_)\n",
    "\n",
    "    # PPI network RWR을 위한 initialize\n",
    "    initialized = RWR_initializing(G, seed_nodes=target_node,indirect_target_nodes=indirect_nodes, is_weighted=False) \n",
    "\n",
    "    # RWR 실행\n",
    "    rwr = RWR(initialized=initialized, prob=0.2, max_iter=100, tol=1.0e-6)\n",
    "    rwr_mapping_entrez = [[entrez_list[i], rwr[node_index.get(entrez_list[i])]] for i in range(len(entrez_list))]\n",
    "    \n",
    "    var_name1 = f'{compoundid}_num1'\n",
    "    var_name2 = f'{compoundid}_num2'\n",
    "\n",
    "    # 전역 변수 설정\n",
    "    globals()[var_name1] = num1\n",
    "    globals()[var_name2] = num2\n",
    "    \n",
    "    return rwr_mapping_entrez\n",
    "\n",
    "def rwr_compound_target(count,entrez_list):\n",
    "    \n",
    "    num1=directed_target_gene_list[count]\n",
    "    num2=indirected_target_gene_list_count[count]\n",
    "    \n",
    "    random_interaction_gene1 = random.sample(entrez_list, num1)\n",
    "    remaining_entrez_list = [gene for gene in entrez_list if gene not in random_interaction_gene1]\n",
    "    random_interaction_gene2 = random.sample(remaining_entrez_list, num2)\n",
    "    # restart node에 적용할 target gene List 생성\n",
    "    target_node = [] \n",
    "    for entrez in random_interaction_gene1:\n",
    "        node_ = node_index.get(entrez)\n",
    "        target_node.append(node_)\n",
    "\n",
    "    #indirect nodes 정의\n",
    "    indirect_nodes= []\n",
    "    for entrez in random_interaction_gene2:\n",
    "        node_ = node_index.get(entrez)\n",
    "        indirect_nodes.append(node_)\n",
    "\n",
    "    # PPI network RWR을 위한 initialize\n",
    "    initialized = RWR_initializing(G, seed_nodes=target_node,indirect_target_nodes=indirect_nodes, is_weighted=False) \n",
    "    \n",
    "    # RWR 실행\n",
    "    rwr=RWR(initialized=initialized, prob=0.2, max_iter=100, tol=1.0e-6)\n",
    "    rwr_mapping_entrez = [[entrez_list[i], rwr[node_index.get(entrez_list[i])]] for i in range(len(entrez_list))]\n",
    "    return rwr_mapping_entrez\n",
    "\n",
    "def create_ppi_network(network_data):\n",
    "    network_data = network_data.astype({'Entrez Gene Interactor A': str,\n",
    "                                        'Entrez Gene Interactor B': str})\n",
    "\n",
    "    # symbol list를 생성하고 unique 값만 가지도록 생성\n",
    "    symbolA = network_data.loc[:, 'Entrez Gene Interactor A'].to_list()\n",
    "    symbolB = network_data.loc[:, 'Entrez Gene Interactor B'].to_list()\n",
    "\n",
    "    symbol_list = symbolA + symbolB \n",
    "    symbol_list = set(symbol_list) \n",
    "    symbol_list = list(symbol_list)\n",
    "\n",
    "    # 모든 symbol에 index를 딕셔너리에서 부여\n",
    "    node_index = {}\n",
    "    for i in range(len(symbol_list)):\n",
    "        node_index[symbol_list[i]] = i\n",
    "\n",
    "    # index로 node_list 생성\n",
    "    node_list = node_index.values()\n",
    "    \n",
    "    # index로 edge_list 생성\n",
    "    edge_list = network_data[['Entrez Gene Interactor A', 'Entrez Gene Interactor B']].values.tolist()  #\n",
    "    for i in range(len(edge_list)):\n",
    "        edge_list[i][0] = node_index.get(edge_list[i][0])\n",
    "        edge_list[i][1] = node_index.get(edge_list[i][1])\n",
    "    \n",
    "    # 무방향 그래프 오브젝트 생성\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 그래프에 노드 추가\n",
    "    G.add_nodes_from(node_list)\n",
    "\n",
    "    # 그래프에 엣지 추가\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    print(f'Number of created nodes: {G.number_of_nodes()}')\n",
    "    print(f'Number of created edges: {G.number_of_edges()}')\n",
    "\n",
    "    return G, node_index\n",
    "\n",
    "def RWR_initializing(G, seed_nodes, indirect_target_nodes, is_weighted=False):\n",
    "    norm_A = np.zeros(shape=(len(G), len(G)))\n",
    "    if is_weighted:  \n",
    "        for i, neighbor_dict in G.adjacency():\n",
    "            for j, v in neighbor_dict.items():\n",
    "                norm_A[i][j] = v.get(\"weight\", 1 / len(neighbor_dict))\n",
    "    else:  \n",
    "        for i, neighbor_dict in G.adjacency():  \n",
    "            for j, v in neighbor_dict.items():  \n",
    "                norm_A[i][j] = 1 / len(neighbor_dict)\n",
    "    \n",
    "    # seed nodes와 indirect target nodes를 모두 포함한 딕셔너리 생성\n",
    "    personalization = {node: 1 for node in seed_nodes}\n",
    "    for node in indirect_target_nodes:\n",
    "        personalization[node] = 0.3  \n",
    "        \n",
    "    r_0 = np.array([personalization.get(n, 0) for n in range(len(G))]) \n",
    "    r_c = np.repeat(1 / len(G), len(G)) \n",
    "    return {\"norm_A\": norm_A, \"r_0\": r_0, \"r_c\": r_c, \"N\": len(G)}\n",
    "\n",
    "def RWR(initialized, prob=0.85, max_iter=100, tol=1.0e-6):\n",
    "    if initialized is None:\n",
    "        raise ValueError('initialized information must be required')\n",
    "\n",
    "    norm_A = initialized['norm_A']  # 인접행렬 A (정규화된)\n",
    "    r_0 = initialized['r_0']        # 시작 노드\n",
    "    r_c = initialized['r_c']        # 가중치 행렬\n",
    "    N = initialized['N']            # 노드의 개수\n",
    "    for iteration in range(max_iter):\n",
    "        r_prev = r_c\n",
    "        r_c = prob * r_c @ norm_A + (1 - prob) * r_0  # RWR algorithm\n",
    "        err = np.absolute(r_c - r_prev).sum()\n",
    "        if err < N * tol:\n",
    "#             print(f'RWR iteration = {iteration + 1}')\n",
    "#             print('Converged')\n",
    "            return r_c\n",
    "#         else:\n",
    "#             print(f'RWR iteration = {iteration + 1}, Iteration until convergence ...')\n",
    "#             print(f'{err} -> {N * tol}')\n",
    "    return \"NotConverged\"\n",
    "\n",
    "# BIOGRID-Homo_sapiens Data 기반 PPI Network 구축\n",
    "network_data = pd.read_csv(\"/data/home/sss2061/식약처/Data/BIOGRID-ORGANISM-Homo_sapiens.tab3.txt\", sep=\"\\t\", encoding=\"cp949\")  \n",
    "G, node_index = create_ppi_network(network_data=network_data)  \n",
    "entrez_list = list(node_index.keys())\n",
    "entrez_list.remove('-')\n",
    "gene_phenotype_file = '/data/home/sss2061/식약처보고서/서울과기대/Data/HPO_gene_phenotype.csv'\n",
    "gene_phenotype_df= pd.read_csv(gene_phenotype_file, encoding='UTF-8', converters={'Phenotype': ast.literal_eval})\n",
    "# 각 phenotype에 대한 Entrez ID들의 리스트를 만들기 위한 빈 딕셔너리\n",
    "phenotype_entrez_dict = {}\n",
    "\n",
    "#리스트 내 요소 문자형 컴마 -> 정수형으로 변경\n",
    "entrez_list = [int(item) for item in entrez_list]\n",
    "\n",
    "# 각 행에 대해 반복\n",
    "for index, row in gene_phenotype_df.iterrows():\n",
    "    entrez_id = row['Entrez ID']\n",
    "    if entrez_id in entrez_list:\n",
    "        for phenotype in row['Phenotype']:\n",
    "            if phenotype in phenotype_entrez_dict:\n",
    "                phenotype_entrez_dict[phenotype].add(entrez_id)\n",
    "            else:\n",
    "                phenotype_entrez_dict[phenotype] = {entrez_id}\n",
    "\n",
    "# 딕셔너리에서 데이터프레임으로 변환\n",
    "final_df = pd.DataFrame([(key, list(values)) for key, values in phenotype_entrez_dict.items()], \n",
    "                        columns=['Phenotype', 'Entrez ID'])\n",
    "final_df.to_csv('phenotype_entrezid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwr3",
   "language": "python",
   "name": "rwr3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
