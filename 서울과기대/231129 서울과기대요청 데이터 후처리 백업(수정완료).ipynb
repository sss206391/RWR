{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e28b647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 후처리\n",
    "# 기존 111번까지는 phenotype / score / z_score / p_value 파일 존재\n",
    "# 이후 score scaling\n",
    "# 이후 p_value 0.001 ~ 1\n",
    "# 이후 \"수정_id2\" 파일을 통해 phenotype 컬럼을 기준으로 일치하면 umlsid, hpoid 컬럼 추가\n",
    "# 이후 \"phenotype mappingv4\" 파일을 통해 umlis 컬럼을 기준으로 일치하면 group, groupid 추가\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "number=7 #수정 필요\n",
    "\n",
    "with open(\"result/{}차/output_dict.pickle\".format(number),\"rb\") as fi:\n",
    "    output_dict = pickle.load(fi)\n",
    "    \n",
    "# validation_df = pd.read_csv('result/{}차/{}_검증.csv'.format(number,number))\n",
    "inferred_phenotype2 = pd.read_csv('result/{}차/{}_network_score.csv'.format(number,number))\n",
    "\n",
    "phenotype_rwr_score_dict = pd.Series(inferred_phenotype2.Network_score.values, index=inferred_phenotype2.Phenotype).to_dict()\n",
    "\n",
    "temp=list(phenotype_rwr_score_dict.keys())\n",
    "for i in temp:\n",
    "    if np.isnan(phenotype_rwr_score_dict[i])==True:\n",
    "        phenotype_rwr_score_dict.pop(i)\n",
    "\n",
    "Phenotype_score_Phenotype=list(phenotype_rwr_score_dict.keys())\n",
    "Phenotype_score_Score=list(phenotype_rwr_score_dict.values())\n",
    "\n",
    "temp2=list(output_dict.keys())\n",
    "for i in temp2:\n",
    "    if np.isnan(output_dict[i][0])==True:\n",
    "        output_dict.pop(i)\n",
    "        \n",
    "P_value_Phenotype=list(output_dict.keys())\n",
    "P_value_Score=list(output_dict.values())\n",
    "\n",
    "count=0\n",
    "z_score_list=[]\n",
    "outlier_list=[] #제거가능\n",
    "P_value_list=[]\n",
    "rank_number=0\n",
    "result_phenotype=[]\n",
    "result_phenotype_score=[]\n",
    "\n",
    "for i in range(len(P_value_Score)):\n",
    "    Numberlist=P_value_Score[i].copy()\n",
    "    tmp1=P_value_Phenotype[i]\n",
    "    tmp2=Phenotype_score_Phenotype.index(tmp1)\n",
    "    tmp3=Phenotype_score_Score[tmp2]\n",
    "    Numberlist.append(tmp3)\n",
    "    result_phenotype.append(tmp1)\n",
    "    result_phenotype_score.append(tmp3)\n",
    "    \n",
    "    # 평균 계산\n",
    "    mean = calculate_mean(Numberlist)\n",
    "    # 표준편차 계산\n",
    "    std_deviation = calculate_standard_deviation(Numberlist)\n",
    "\n",
    "    z_score=(tmp3-mean)/std_deviation\n",
    "    z_score_list.append(z_score)\n",
    "    if z_score >=-2.58 and z_score<=2.58: #제거가능\n",
    "        outlier=1\n",
    "        count=count+1\n",
    "    else:\n",
    "        outlier=0 \n",
    "    outlier_list.append(outlier) #제거가능\n",
    "\n",
    "    Numberlist.sort(reverse=True)\n",
    "\n",
    "    rank=Numberlist.index(tmp3) #순위찾기\n",
    "    if rank<5:\n",
    "        rank_number=rank_number+1\n",
    "    P_value_list.append(rank)\n",
    "#     print(\"phe\",tmp1)\n",
    "#     print(\"z\",z_score)\n",
    "#     print(\"p\",rank)\n",
    "\n",
    "# postgresql DB 연동\n",
    "config = configparser.ConfigParser()\n",
    "config.read('db_config.ini')\n",
    "conn=psycopg2.connect(host=\"168.131.30.66\", dbname=\"coconut\",user=\"dbuser\",password=\"jnudl1\") #데이터이름변경\n",
    "conn_cur = conn.cursor()\n",
    "\n",
    "#------------------------\n",
    "\n",
    "Phenotype_score = pd.DataFrame({\n",
    "    'Phenotype': result_phenotype,\n",
    "    'Network_score': result_phenotype_score,\n",
    "    'z-score': z_score_list,\n",
    "    'P-value': P_value_list,\n",
    "})\n",
    "\n",
    "# print(Phenotype_score)\n",
    "\n",
    "Phenotype_score=Phenotype_score.sort_values(by='P-value',ascending=True)\n",
    "\n",
    "#     # Compound ID에 해당하는 inferred phenotype.csv 생성\n",
    "# if not os.path.exists('compound 결과'):\n",
    "#     os.makedirs('compound 결과')\n",
    "Phenotype_score.to_csv('result/{}차/{}_검증.csv'.format(number,number),index=False, encoding='UTF-8')\n",
    "\n",
    "#------------------------------------\n",
    "# 'Network_score' 컬럼의 데이터를 추출합니다.\n",
    "network_scores = Phenotype_score['Network_score']\n",
    "\n",
    "# Min-Max Scaling을 0부터 1로 설정하여 적용합니다.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_scores = scaler.fit_transform(network_scores.values.reshape(-1, 1))  # 2D 배열로 변환 후 스케일링\n",
    "\n",
    "# 스케일링된 값을 'scaling_score' 컬럼에 추가합니다.\n",
    "Phenotype_score['scaling_score'] = scaled_scores\n",
    "\n",
    "# 'P-value'의 값을 조정합니다.\n",
    "def adjust_pvalue(value):\n",
    "    if value == 999 or value == 1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return (value+1) / 1000\n",
    "\n",
    "Phenotype_score['P-value'] = Phenotype_score['P-value'].apply(adjust_pvalue)\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "#umls db에서 umls id와 phenotype score 파일을 합치는 코드\n",
    "\n",
    "file_2= pd.read_csv(\"Data/수정_Id2.csv\")\n",
    "\n",
    "# file과 file_2를 'Phenotype'과 'phenotype' 컬럼을 기준으로 결합합니다.\n",
    "# 이때 file_2의 'umlsid' 컬럼만 선택하여 추가합니다.\n",
    "file_3 = pd.merge(Phenotype_score, file_2[['umlsid', 'phenotype','hpoid']], left_on='Phenotype', right_on='phenotype', how='left')\n",
    "\n",
    "# 중복된 'phenotype' 컬럼을 제거합니다.\n",
    "file_3.drop(columns=['phenotype'], inplace=True)\n",
    "Phenotype_score=file_3\n",
    "# print(Phenotype_score)\n",
    "#------------------------------------\n",
    "# 'phenotype mappingv3.xlsx' 파일을 읽어옵니다.\n",
    "mapping_df = pd.read_excel('Data/phenotype mappingv4.xlsx', engine='openpyxl')\n",
    "\n",
    "# '1_검증.csv' 파일 내의 UMLSID와 'phenotype mappingv3.xlsx' 파일 내의 UMLSID를 기반으로 병합합니다.\n",
    "grouped_mapping_df = mapping_df.groupby('UMLSID')['Group'].agg(lambda x: ','.join(x)).reset_index()\n",
    "\n",
    "# Phenotype_score와 grouped_mapping_df를 병합합니다.\n",
    "final_df = pd.merge(Phenotype_score, grouped_mapping_df, left_on='umlsid', right_on='UMLSID', how='left')\n",
    "\n",
    "# 필요없는 UMLSID 컬럼을 제거합니다.\n",
    "final_df.drop(columns=['UMLSID'], inplace=True)\n",
    "\n",
    "cols_to_drop = [col for col in final_df.columns if col.startswith(\"Unnamed\")]\n",
    "final_df = final_df.drop(columns=cols_to_drop)\n",
    "\n",
    "final_df.to_csv('result/{}차/{}_검증_with_scaling.csv'.format(number,number), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number=1\n",
    "# text=[\"adenine\", \"canavanin\", \"malate\", \"isoleucine\", \"phenylalanine\", \"tyrosine\", \"isocitrate\", \"glutamine\", \"tryptophan\",\"L-malate\", \n",
    "#       \"theogallin\", \"trans-aconitate\", \"cis-aconitate\", \"threonate\", \"benzoic acid\", \"choline\", \"citric acid\", \"coumarin\", \"phloroglucinol\", \"gallic acid\", \n",
    "#       \"citrullin\", \"pipecolic acid\", \"vitamin B\", \"pantothenic acid\", \"DL-valine\", \"vanillin\", \"theophylline\", \"caffeine\", \"protocatechuic aldehyde\", \"adenosine\", \n",
    "#       \"EGCG\", \"gallocatechin\", \"N-acetylglutamate\", \"loliolide\", \"procyanidin B2\", \"3H-proline\", \"dihydromyricetin\", \"beta-alanine betaine\", \"epicatechin gallate (ECG\", \"5'-methylthioadenosine\", \n",
    "#       \"epiafzelechin\", \"ferulic acid\", \"p-coumaric acid\", \"caffeic acid\", \"chlorogenic acid\", \"quercetin\", \"alpha-isopropylmalate\", \"neochlorogenic acid\", \"rutin\", \"hyperoside\", \n",
    "#       \"4-p-coumaroylquinic acid\", \"astragalin\", \"quercetin 7-O-glucoside\", \"p-coumaroylquinic acid\", \"procyanidin B1\"]\n",
    "\n",
    "# number=2\n",
    "# text=[\"gamma-aminobutyric acid (GABA\", \"canavanin\", \"sucrose\", \"folate\", \"guanosine\", \"vanillic acid\", \"ononin\", \n",
    "#       \"formononetin\", \"calycosin\", \"sissotrin\", \"afrormosin\", \"pratensein\"]\n",
    "\n",
    "# number=3\n",
    "# text=[\"protocatechuic acid\", \"gamma-aminobutyric acid (GABA\", \"choline\", \"citric acid\", \"malate\", \"asparagine\", \n",
    "#       \"quinic acid\", \"maltol\", \"nodakenetin\", \"nodakenin\", \"columbianetin\", \"decursin\", \"ferulic acid\", \"caffeic acid\",\n",
    "#       \"aegelinol\", \"cis-chlorogenic acid\", \"chlorogenic acid\", \"esculetin\"]\n",
    "\n",
    "# number=4\n",
    "# text=[\"piperine\", \"piperanine\", \"piperlonguminine\", \"retrofractamide B\", \"dehydropipernonaline\", \"Methyl piperate\", \"pipernonaline\", \"Retrofractamide A\", \"retrofractamide C\"]\n",
    "\n",
    "# number=5\n",
    "# text=[\"apocynin\", \"resacetophenone\", \"4-hydroxyacetophenone\", \"2,5-dihydroxyacetophenone\", \"physcion\"]\n",
    "\n",
    "# number=6\n",
    "# text=[\"protocatechuic acid\", \"p-hydroxybenzaldehyde\", \"choline\", \"gluconate\", \"myo-inositol\", \"thymine\", \"uracil\", \"vanillin\", \"vanillic acid\", \"protocatechuic aldehyde\", \"trans-aconitate\", \"ferulic acid\", \"caffeic acid\", \"chlorogenic acid\", \"linoleic acid\", \"neochlorogenic acid\", \"xanthatin\", \"cynarin\", \"cynarine\"]\n",
    "\n",
    "# number=7\n",
    "# text=[\"atractylenolide III\", \"vanillic acid\", \"esculetin\", \"elemicin\", \"adenosine\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba949dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HPO gene_phenotype 데이터와 PPI network 내에서 entrez_list들의 교집합을 phenotype / entrez_id 형식으로 변환\n",
    "\n",
    "# herb 최종\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def calculate_mean(numbers):\n",
    "    return sum(numbers) / len(numbers)\n",
    "\n",
    "def calculate_standard_deviation(numbers):\n",
    "    return statistics.stdev(numbers)\n",
    "\n",
    "def combine_lists(full_list):\n",
    "    combined = defaultdict(list)\n",
    "    \n",
    "    # 모든 리스트에 대한 처리\n",
    "    for sublist in full_list:\n",
    "        for item in sublist:\n",
    "            entrez_id, rwr_score = item\n",
    "            combined[entrez_id].append(rwr_score)\n",
    "    \n",
    "    # 조화평균 및 결과 리스트 만들기\n",
    "    result = []\n",
    "    for entrez_id, scores in combined.items():\n",
    "        if len(scores) == 1:\n",
    "            result.append([entrez_id, scores[0]])  # 값이 하나면 그대로 추가\n",
    "        else:\n",
    "            # 조화평균 계산\n",
    "            harmonic_mean_score = harmonic_mean(scores)\n",
    "            result.append([entrez_id, harmonic_mean_score])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compound_target(compoundid):\n",
    "\n",
    "    conn_cur.execute(f\"with tmp1 as (select cg.geneid from compound_gene cg where (cg.compid = '{compoundid}' and cg.relationsign in ('activate','inhibit')))\" \n",
    "                    f\"select g.entrezid from gene g,tmp1 where g.geneid=tmp1.geneid;\")\n",
    "    compound_gene1=conn_cur.fetchall()\n",
    "\n",
    "    compound_gene=set(compound_gene1)\n",
    "    compound_gene=list(compound_gene)\n",
    "    \n",
    "    # Compound-target gene List 생성\n",
    "    target_gene_list = [] \n",
    "    for i in range(len(compound_gene)):\n",
    "        interaction_gene = compound_gene[i][0]\n",
    "        if interaction_gene in entrez_list:\n",
    "            target_gene_list.append(interaction_gene)        \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # restart node에 적용할 target gene List 생성\n",
    "    target_node = [] \n",
    "    for entrez in target_gene_list:\n",
    "        node_ = node_index.get(entrez) \n",
    "        target_node.append(node_)\n",
    "\n",
    "    conn_cur.execute(f\"with tmp2 as (select cg.geneid from compound_gene cg where cg.compid = '{compoundid}' and relationsign='associate')\" \n",
    "                    f\"select g.entrezid from gene g,tmp2 where g.geneid=tmp2.geneid;\")\n",
    "\n",
    "    compound_gene2=conn_cur.fetchall()\n",
    "\n",
    "    compound_gene=set(compound_gene2)\n",
    "    compound_gene=list(compound_gene)\n",
    "\n",
    "    indirected_target_gene_list = [] \n",
    "    for i in range(len(compound_gene)):\n",
    "        interaction_gene = compound_gene[i][0]\n",
    "        if interaction_gene in entrez_list:\n",
    "            indirected_target_gene_list.append(interaction_gene)        \n",
    "        else:\n",
    "            continue      \n",
    "\n",
    "    num1=len(target_gene_list) #target gene list 갯수\n",
    "    directed_target_gene_list.append(num1)\n",
    "    num2=len(indirected_target_gene_list) #indirected target gene list 갯수 \n",
    "    indirected_target_gene_list_count.append(num2)\n",
    "    \n",
    "    #indirect nodes 정의\n",
    "    indirect_nodes= []\n",
    "    for entrez in indirected_target_gene_list:\n",
    "        node_ = node_index.get(entrez) \n",
    "        indirect_nodes.append(node_)\n",
    "\n",
    "    # PPI network RWR을 위한 initialize\n",
    "    initialized = RWR_initializing(G, seed_nodes=target_node,indirect_target_nodes=indirect_nodes, is_weighted=False) \n",
    "\n",
    "    # RWR 실행\n",
    "    rwr = RWR(initialized=initialized, prob=0.2, max_iter=100, tol=1.0e-6)\n",
    "    rwr_mapping_entrez = [[entrez_list[i], rwr[node_index.get(entrez_list[i])]] for i in range(len(entrez_list))]\n",
    "    \n",
    "    var_name1 = f'{compoundid}_num1'\n",
    "    var_name2 = f'{compoundid}_num2'\n",
    "\n",
    "    # 전역 변수 설정\n",
    "    globals()[var_name1] = num1\n",
    "    globals()[var_name2] = num2\n",
    "    \n",
    "    return rwr_mapping_entrez\n",
    "\n",
    "def rwr_compound_target(count,entrez_list):\n",
    "    \n",
    "    num1=directed_target_gene_list[count]\n",
    "    num2=indirected_target_gene_list_count[count]\n",
    "    \n",
    "    random_interaction_gene1 = random.sample(entrez_list, num1)\n",
    "    remaining_entrez_list = [gene for gene in entrez_list if gene not in random_interaction_gene1]\n",
    "    random_interaction_gene2 = random.sample(remaining_entrez_list, num2)\n",
    "    # restart node에 적용할 target gene List 생성\n",
    "    target_node = [] \n",
    "    for entrez in random_interaction_gene1:\n",
    "        node_ = node_index.get(entrez)\n",
    "        target_node.append(node_)\n",
    "\n",
    "    #indirect nodes 정의\n",
    "    indirect_nodes= []\n",
    "    for entrez in random_interaction_gene2:\n",
    "        node_ = node_index.get(entrez)\n",
    "        indirect_nodes.append(node_)\n",
    "\n",
    "    # PPI network RWR을 위한 initialize\n",
    "    initialized = RWR_initializing(G, seed_nodes=target_node,indirect_target_nodes=indirect_nodes, is_weighted=False) \n",
    "    \n",
    "    # RWR 실행\n",
    "    rwr=RWR(initialized=initialized, prob=0.2, max_iter=100, tol=1.0e-6)\n",
    "    rwr_mapping_entrez = [[entrez_list[i], rwr[node_index.get(entrez_list[i])]] for i in range(len(entrez_list))]\n",
    "    return rwr_mapping_entrez\n",
    "\n",
    "def create_ppi_network(network_data):\n",
    "    network_data = network_data.astype({'Entrez Gene Interactor A': str,\n",
    "                                        'Entrez Gene Interactor B': str})\n",
    "\n",
    "    # symbol list를 생성하고 unique 값만 가지도록 생성\n",
    "    symbolA = network_data.loc[:, 'Entrez Gene Interactor A'].to_list()\n",
    "    symbolB = network_data.loc[:, 'Entrez Gene Interactor B'].to_list()\n",
    "\n",
    "    symbol_list = symbolA + symbolB \n",
    "    symbol_list = set(symbol_list) \n",
    "    symbol_list = list(symbol_list)\n",
    "\n",
    "    # 모든 symbol에 index를 딕셔너리에서 부여\n",
    "    node_index = {}\n",
    "    for i in range(len(symbol_list)):\n",
    "        node_index[symbol_list[i]] = i\n",
    "\n",
    "    # index로 node_list 생성\n",
    "    node_list = node_index.values()\n",
    "    \n",
    "    # index로 edge_list 생성\n",
    "    edge_list = network_data[['Entrez Gene Interactor A', 'Entrez Gene Interactor B']].values.tolist()  #\n",
    "    for i in range(len(edge_list)):\n",
    "        edge_list[i][0] = node_index.get(edge_list[i][0])\n",
    "        edge_list[i][1] = node_index.get(edge_list[i][1])\n",
    "    \n",
    "    # 무방향 그래프 오브젝트 생성\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 그래프에 노드 추가\n",
    "    G.add_nodes_from(node_list)\n",
    "\n",
    "    # 그래프에 엣지 추가\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    print(f'Number of created nodes: {G.number_of_nodes()}')\n",
    "    print(f'Number of created edges: {G.number_of_edges()}')\n",
    "\n",
    "    return G, node_index\n",
    "\n",
    "def RWR_initializing(G, seed_nodes, indirect_target_nodes, is_weighted=False):\n",
    "    norm_A = np.zeros(shape=(len(G), len(G)))\n",
    "    if is_weighted:  \n",
    "        for i, neighbor_dict in G.adjacency():\n",
    "            for j, v in neighbor_dict.items():\n",
    "                norm_A[i][j] = v.get(\"weight\", 1 / len(neighbor_dict))\n",
    "    else:  \n",
    "        for i, neighbor_dict in G.adjacency():  \n",
    "            for j, v in neighbor_dict.items():  \n",
    "                norm_A[i][j] = 1 / len(neighbor_dict)\n",
    "    \n",
    "    # seed nodes와 indirect target nodes를 모두 포함한 딕셔너리 생성\n",
    "    personalization = {node: 1 for node in seed_nodes}\n",
    "    for node in indirect_target_nodes:\n",
    "        personalization[node] = 0.3  \n",
    "        \n",
    "    r_0 = np.array([personalization.get(n, 0) for n in range(len(G))]) \n",
    "    r_c = np.repeat(1 / len(G), len(G)) \n",
    "    return {\"norm_A\": norm_A, \"r_0\": r_0, \"r_c\": r_c, \"N\": len(G)}\n",
    "\n",
    "def RWR(initialized, prob=0.85, max_iter=100, tol=1.0e-6):\n",
    "    if initialized is None:\n",
    "        raise ValueError('initialized information must be required')\n",
    "\n",
    "    norm_A = initialized['norm_A']  # 인접행렬 A (정규화된)\n",
    "    r_0 = initialized['r_0']        # 시작 노드\n",
    "    r_c = initialized['r_c']        # 가중치 행렬\n",
    "    N = initialized['N']            # 노드의 개수\n",
    "    for iteration in range(max_iter):\n",
    "        r_prev = r_c\n",
    "        r_c = prob * r_c @ norm_A + (1 - prob) * r_0  # RWR algorithm\n",
    "        err = np.absolute(r_c - r_prev).sum()\n",
    "        if err < N * tol:\n",
    "#             print(f'RWR iteration = {iteration + 1}')\n",
    "#             print('Converged')\n",
    "            return r_c\n",
    "#         else:\n",
    "#             print(f'RWR iteration = {iteration + 1}, Iteration until convergence ...')\n",
    "#             print(f'{err} -> {N * tol}')\n",
    "    return \"NotConverged\"\n",
    "\n",
    "# BIOGRID-Homo_sapiens Data 기반 PPI Network 구축\n",
    "network_data = pd.read_csv(\"/data/home/sss2061/식약처/Data/BIOGRID-ORGANISM-Homo_sapiens.tab3.txt\", sep=\"\\t\", encoding=\"cp949\")  \n",
    "G, node_index = create_ppi_network(network_data=network_data)  \n",
    "entrez_list = list(node_index.keys())\n",
    "entrez_list.remove('-')\n",
    "gene_phenotype_file = '/data/home/sss2061/식약처보고서/서울과기대/Data/HPO_gene_phenotype.csv'\n",
    "gene_phenotype_df= pd.read_csv(gene_phenotype_file, encoding='UTF-8', converters={'Phenotype': ast.literal_eval})\n",
    "# 각 phenotype에 대한 Entrez ID들의 리스트를 만들기 위한 빈 딕셔너리\n",
    "phenotype_entrez_dict = {}\n",
    "\n",
    "#리스트 내 요소 문자형 컴마 -> 정수형으로 변경\n",
    "entrez_list = [int(item) for item in entrez_list]\n",
    "\n",
    "# 각 행에 대해 반복\n",
    "for index, row in gene_phenotype_df.iterrows():\n",
    "    entrez_id = row['Entrez ID']\n",
    "    if entrez_id in entrez_list:\n",
    "        for phenotype in row['Phenotype']:\n",
    "            if phenotype in phenotype_entrez_dict:\n",
    "                phenotype_entrez_dict[phenotype].add(entrez_id)\n",
    "            else:\n",
    "                phenotype_entrez_dict[phenotype] = {entrez_id}\n",
    "\n",
    "# 딕셔너리에서 데이터프레임으로 변환\n",
    "final_df = pd.DataFrame([(key, list(values)) for key, values in phenotype_entrez_dict.items()], \n",
    "                        columns=['Phenotype', 'Entrez ID'])\n",
    "final_df.to_csv('phenotype_entrezid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce5dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 후처리\n",
    "# 기존 111번까지는 phenotype / score / z_score / p_value 파일 존재\n",
    "# 이후 score scaling\n",
    "# 이후 p_value 0.001 ~ 1\n",
    "# 이후 \"수정_id2\" 파일을 통해 phenotype 컬럼을 기준으로 일치하면 umlsid, hpoid 컬럼 추가\n",
    "# 이후 \"phenotype mappingv4\" 파일을 통해 umlis 컬럼을 기준으로 일치하면 group, groupid 추가\n",
    "# groupid와 group을 같은 행이 아닌 다른 행에 추가할때 사용하는 코드\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "number=5 #수정 필요\n",
    "\n",
    "with open(\"result/{}차/output_dict.pickle\".format(number),\"rb\") as fi:\n",
    "    output_dict = pickle.load(fi)\n",
    "    \n",
    "# validation_df = pd.read_csv('result/{}차/{}_검증.csv'.format(number,number))\n",
    "inferred_phenotype2 = pd.read_csv('result/{}차/{}_network_score.csv'.format(number,number))\n",
    "\n",
    "phenotype_rwr_score_dict = pd.Series(inferred_phenotype2.Network_score.values, index=inferred_phenotype2.Phenotype).to_dict()\n",
    "\n",
    "temp=list(phenotype_rwr_score_dict.keys())\n",
    "for i in temp:\n",
    "    if np.isnan(phenotype_rwr_score_dict[i])==True:\n",
    "        phenotype_rwr_score_dict.pop(i)\n",
    "\n",
    "Phenotype_score_Phenotype=list(phenotype_rwr_score_dict.keys())\n",
    "Phenotype_score_Score=list(phenotype_rwr_score_dict.values())\n",
    "\n",
    "temp2=list(output_dict.keys())\n",
    "for i in temp2:\n",
    "    if np.isnan(output_dict[i][0])==True:\n",
    "        output_dict.pop(i)\n",
    "        \n",
    "P_value_Phenotype=list(output_dict.keys())\n",
    "P_value_Score=list(output_dict.values())\n",
    "\n",
    "count=0\n",
    "z_score_list=[]\n",
    "outlier_list=[] #제거가능\n",
    "P_value_list=[]\n",
    "rank_number=0\n",
    "result_phenotype=[]\n",
    "result_phenotype_score=[]\n",
    "\n",
    "for i in range(len(P_value_Score)):\n",
    "    Numberlist=P_value_Score[i].copy()\n",
    "    tmp1=P_value_Phenotype[i]\n",
    "    tmp2=Phenotype_score_Phenotype.index(tmp1)\n",
    "    tmp3=Phenotype_score_Score[tmp2]\n",
    "    Numberlist.append(tmp3)\n",
    "    result_phenotype.append(tmp1)\n",
    "    result_phenotype_score.append(tmp3)\n",
    "    \n",
    "    # 평균 계산\n",
    "    mean = calculate_mean(Numberlist)\n",
    "    # 표준편차 계산\n",
    "    std_deviation = calculate_standard_deviation(Numberlist)\n",
    "    if std_deviation==0:    #제거\n",
    "        z_score=0\n",
    "    else:\n",
    "        z_score=(tmp3-mean)/std_deviation\n",
    "\n",
    "    z_score_list.append(z_score)\n",
    "    if z_score >=-2.58 and z_score<=2.58: #제거가능\n",
    "        outlier=1\n",
    "        count=count+1\n",
    "    else:\n",
    "        outlier=0 \n",
    "    outlier_list.append(outlier) #제거가능\n",
    "\n",
    "    Numberlist.sort(reverse=True)\n",
    "\n",
    "    rank=Numberlist.index(tmp3) #순위찾기\n",
    "    if rank<5:\n",
    "        rank_number=rank_number+1\n",
    "    P_value_list.append(rank)\n",
    "#     print(\"phe\",tmp1)\n",
    "#     print(\"z\",z_score)\n",
    "#     print(\"p\",rank)\n",
    "\n",
    "# postgresql DB 연동\n",
    "config = configparser.ConfigParser()\n",
    "config.read('db_config.ini')\n",
    "conn=psycopg2.connect(host=\"168.131.30.66\", dbname=\"coconut\",user=\"dbuser\",password=\"jnudl1\") #데이터이름변경\n",
    "conn_cur = conn.cursor()\n",
    "\n",
    "#------------------------\n",
    "\n",
    "Phenotype_score = pd.DataFrame({\n",
    "    'Phenotype': result_phenotype,\n",
    "    'Network_score': result_phenotype_score,\n",
    "    'z-score': z_score_list,\n",
    "    'P-value': P_value_list,\n",
    "})\n",
    "\n",
    "# print(Phenotype_score)\n",
    "\n",
    "Phenotype_score=Phenotype_score.sort_values(by='P-value',ascending=True)\n",
    "\n",
    "#     # Compound ID에 해당하는 inferred phenotype.csv 생성\n",
    "# if not os.path.exists('compound 결과'):\n",
    "#     os.makedirs('compound 결과')\n",
    "Phenotype_score.to_csv('result/{}차/{}_검증.csv'.format(number,number),index=False, encoding='UTF-8')\n",
    "\n",
    "#------------------------------------\n",
    "# 'Network_score' 컬럼의 데이터를 추출합니다.\n",
    "network_scores = Phenotype_score['Network_score']\n",
    "\n",
    "# Min-Max Scaling을 0부터 1로 설정하여 적용합니다.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_scores = scaler.fit_transform(network_scores.values.reshape(-1, 1))  # 2D 배열로 변환 후 스케일링\n",
    "\n",
    "# 스케일링된 값을 'scaling_score' 컬럼에 추가합니다.\n",
    "Phenotype_score['scaling_score'] = scaled_scores\n",
    "\n",
    "# 'P-value'의 값을 조정합니다.\n",
    "def adjust_pvalue(value):\n",
    "    if value == 999 or value == 1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return (value+1) / 1000\n",
    "\n",
    "Phenotype_score['P-value'] = Phenotype_score['P-value'].apply(adjust_pvalue)\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "#umls db에서 umls id와 phenotype score 파일을 합치는 코드\n",
    "\n",
    "file_2= pd.read_csv(\"Data/수정_Id2.csv\")\n",
    "\n",
    "# file과 file_2를 'Phenotype'과 'phenotype' 컬럼을 기준으로 결합합니다.\n",
    "# 이때 file_2의 'umlsid' 컬럼만 선택하여 추가합니다.\n",
    "file_3 = pd.merge(Phenotype_score, file_2[['umlsid', 'phenotype','hpoid']], left_on='Phenotype', right_on='phenotype', how='left')\n",
    "\n",
    "# 중복된 'phenotype' 컬럼을 제거합니다.\n",
    "file_3.drop(columns=['phenotype'], inplace=True)\n",
    "Phenotype_score=file_3\n",
    "# print(Phenotype_score)\n",
    "#------------------------------------\n",
    "# 'phenotype mappingv3.xlsx' 파일을 읽어옵니다.\n",
    "mapping_df = pd.read_excel('Data/phenotype mappingv4.xlsx', engine='openpyxl')\n",
    "\n",
    "merged_df = pd.merge(Phenotype_score, mapping_df, left_on='umlsid', right_on='UMLSID', how='left')\n",
    "\n",
    "# 필요없는 UMLSID 컬럼을 제거합니다.\n",
    "merged_df.drop(columns=['UMLSID'], inplace=True)\n",
    "\n",
    "cols_to_drop = [col for col in merged_df.columns if col.startswith(\"Unnamed\")]\n",
    "final_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "final_df.to_csv('result/{}차/{}_검증_with_scaling_버전2.csv'.format(number,number), index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwr3",
   "language": "python",
   "name": "rwr3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
