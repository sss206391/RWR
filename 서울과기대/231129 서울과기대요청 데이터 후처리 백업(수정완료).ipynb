{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0664eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀파일이 'phenotype mappingv3.xlsx', '1_검증.csv' 이렇게 2개가 있어\n",
    "#  'phenotype mappingv3.xlsx' 파일은 UMLSID, Group, GroupID 이렇게 3개의 컬럼이 존재하고\n",
    "# '1_검증.csv' 파일은 Phenotype, Network_score, z-score, P-value, UMLSID 이렇게 5개의 컬럼이 존재해\n",
    "# 여기서 '1_검증.csv' 파일 내에 있는 UMLSID의 데이터와  'phenotype mappingv3.xlsx' 파일 내에 있는 UMLSID의 데이터가 일치하면 Group, GroupID 값을 가져와서 똑같이 써주고, 아니면 None 값을 써주고 싶어\n",
    "# 그래서 최종적으로 '1_검증.csv' 파일내에 Phenotype, Network_score, z-score, P-value, UMLSID, Group, GroupID 이렇게 7개의 컬럼이 존재하게 만들고 싶어\n",
    "# 이렇게 코드 짜줘\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# phenotype_mapping_file='/data/home/sss2061/식약처보고서/서울과기대/phenotype mappingv3.xlsx'\n",
    "\n",
    "# '1_검증.csv' 파일을 읽어옵니다.\n",
    "\n",
    "number=2 #수정 필요\n",
    "validation_df = pd.read_csv('result/{}차/{}_검증.csv'.format(number,number)) #수정 필요\n",
    "\n",
    "# 'phenotype mappingv3.xlsx' 파일을 읽어옵니다.\n",
    "mapping_df = pd.read_excel('phenotype mappingv3.xlsx', engine='openpyxl')\n",
    "\n",
    "# '1_검증.csv' 파일 내의 UMLSID와 'phenotype mappingv3.xlsx' 파일 내의 UMLSID를 기반으로 병합합니다.\n",
    "merged_df = validation_df.merge(mapping_df, on='UMLSID', how='left')\n",
    "\n",
    "# Group 및 GroupID가 없는 경우 None으로 채웁니다.\n",
    "merged_df['Group'].fillna('None', inplace=True)\n",
    "merged_df['GroupID'].fillna('None', inplace=True)\n",
    "\n",
    "# 'Network_score' 컬럼의 데이터를 추출합니다.\n",
    "network_scores = merged_df['Network_score']\n",
    "\n",
    "# Min-Max Scaling을 0부터 1로 설정하여 적용합니다.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_scores = scaler.fit_transform(network_scores.values.reshape(-1, 1))  # 2D 배열로 변환 후 스케일링\n",
    "\n",
    "# 스케일링된 값을 'scaling_score' 컬럼에 추가합니다.\n",
    "merged_df['scaling_score'] = scaled_scores\n",
    "\n",
    "# 'P-value'의 값을 조정합니다.\n",
    "def adjust_pvalue(value):\n",
    "    if value == 999 or value == 1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return (value+1) / 1000\n",
    "\n",
    "merged_df['P-value'] = merged_df['P-value'].apply(adjust_pvalue)\n",
    "\n",
    "cols_to_drop = [col for col in merged_df.columns if col.startswith(\"Unnamed\")]\n",
    "merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "hpo_df=pd.read_csv(\"Data/[original] HPO phenotype data.csv\")\n",
    "\n",
    "merged_df=pd.merge(merged_df, hpo_df, left_on='Phenotype', right_on='phenotype name', how='left')\n",
    "\n",
    "final_df=merged_df[['Phenotype','Network_score','z-score','P-value','UMLSID','Group','GroupID','scaling_score', 'hpo_id']]\n",
    "\n",
    "# 수정된 DataFrame을 새로운 엑셀 파일로 저장합니다.\n",
    "final_df.to_csv('result/{}차/{}_검증_with_scaling.csv'.format(number,number), index=False)  # 저장할 파일명을 지정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184a891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 백업본\n",
    "\n",
    "# 엑셀파일이 'phenotype mappingv3.xlsx', '1_검증.csv' 이렇게 2개가 있어\n",
    "#  'phenotype mappingv3.xlsx' 파일은 UMLSID, Group, GroupID 이렇게 3개의 컬럼이 존재하고\n",
    "# '1_검증.csv' 파일은 Phenotype, Network_score, z-score, P-value, UMLSID 이렇게 5개의 컬럼이 존재해\n",
    "# 여기서 '1_검증.csv' 파일 내에 있는 UMLSID의 데이터와  'phenotype mappingv3.xlsx' 파일 내에 있는 UMLSID의 데이터가 일치하면 Group, GroupID 값을 가져와서 똑같이 써주고, 아니면 None 값을 써주고 싶어\n",
    "# 그래서 최종적으로 '1_검증.csv' 파일내에 Phenotype, Network_score, z-score, P-value, UMLSID, Group, GroupID 이렇게 7개의 컬럼이 존재하게 만들고 싶어\n",
    "# 이렇게 코드 짜줘\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "\n",
    "# phenotype_mapping_file='/data/home/sss2061/식약처보고서/서울과기대/phenotype mappingv3.xlsx'\n",
    "\n",
    "# '1_검증.csv' 파일을 읽어옵니다.\n",
    "\n",
    "number=2\n",
    "validation_df = pd.read_csv('result/{}차/{}_검증.csv'.format(number,number))\n",
    "\n",
    "# 'phenotype mappingv3.xlsx' 파일을 읽어옵니다.\n",
    "mapping_df = pd.read_excel('phenotype mappingv3.xlsx', engine='openpyxl')\n",
    "\n",
    "# '1_검증.csv' 파일 내의 UMLSID와 'phenotype mappingv3.xlsx' 파일 내의 UMLSID를 기반으로 병합합니다.\n",
    "merged_df = validation_df.merge(mapping_df, on='UMLSID', how='left')\n",
    "\n",
    "# Group 및 GroupID가 없는 경우 None으로 채웁니다.\n",
    "merged_df['Group'].fillna('None', inplace=True)\n",
    "merged_df['GroupID'].fillna('None', inplace=True)\n",
    "\n",
    "# 결과를 'merged_1_검증.csv' 파일로 저장합니다.\n",
    "merged_df.to_csv('merged_14_검증.csv', index=False)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 'merged_1_검증.xlsx' 파일을 읽어옵니다.\n",
    "df = pd.read_csv('merged_14_검증.csv')  # 파일 경로를 확인해주세요.\n",
    "\n",
    "# 'Network_score' 컬럼의 데이터를 추출합니다.\n",
    "network_scores = df['Network_score']\n",
    "\n",
    "# Min-Max Scaling을 0부터 1로 설정하여 적용합니다.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_scores = scaler.fit_transform(network_scores.values.reshape(-1, 1))  # 2D 배열로 변환 후 스케일링\n",
    "\n",
    "# 스케일링된 값을 'scaling_score' 컬럼에 추가합니다.\n",
    "df['scaling_score'] = scaled_scores\n",
    "\n",
    "# 'P-value'의 값을 조정합니다.\n",
    "def adjust_pvalue(value):\n",
    "    if value == 999 or value == 1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return (value+1) / 1000\n",
    "\n",
    "df['P-value'] = df['P-value'].apply(adjust_pvalue)\n",
    "\n",
    "# \"Unnamed\"로 시작하는 컬럼을 찾아 삭제합니다.\n",
    "cols_to_drop = [col for col in df.columns if col.startswith(\"Unnamed\")]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# 수정된 DataFrame을 새로운 엑셀 파일로 저장합니다.\n",
    "df.to_csv('merged_14_검증_with_scaling.csv', index=False)  # 저장할 파일명을 지정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5c5e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# number=1\n",
    "# text=[\"adenine\", \"canavanin\", \"malate\", \"isoleucine\", \"phenylalanine\", \"tyrosine\", \"isocitrate\", \"glutamine\", \"tryptophan\",\"L-malate\", \n",
    "#       \"theogallin\", \"trans-aconitate\", \"cis-aconitate\", \"threonate\", \"benzoic acid\", \"choline\", \"citric acid\", \"coumarin\", \"phloroglucinol\", \"gallic acid\", \n",
    "#       \"citrullin\", \"pipecolic acid\", \"vitamin B\", \"pantothenic acid\", \"DL-valine\", \"vanillin\", \"theophylline\", \"caffeine\", \"protocatechuic aldehyde\", \"adenosine\", \n",
    "#       \"EGCG\", \"gallocatechin\", \"N-acetylglutamate\", \"loliolide\", \"procyanidin B2\", \"3H-proline\", \"dihydromyricetin\", \"beta-alanine betaine\", \"epicatechin gallate (ECG\", \"5'-methylthioadenosine\", \n",
    "#       \"epiafzelechin\", \"ferulic acid\", \"p-coumaric acid\", \"caffeic acid\", \"chlorogenic acid\", \"quercetin\", \"alpha-isopropylmalate\", \"neochlorogenic acid\", \"rutin\", \"hyperoside\", \n",
    "#       \"4-p-coumaroylquinic acid\", \"astragalin\", \"quercetin 7-O-glucoside\", \"p-coumaroylquinic acid\", \"procyanidin B1\"]\n",
    "\n",
    "# number=2\n",
    "# text=[\"gamma-aminobutyric acid (GABA\", \"canavanin\", \"sucrose\", \"folate\", \"guanosine\", \"vanillic acid\", \"ononin\", \n",
    "#       \"formononetin\", \"calycosin\", \"sissotrin\", \"afrormosin\", \"pratensein\"]\n",
    "\n",
    "# number=3\n",
    "# text=[\"protocatechuic acid\", \"gamma-aminobutyric acid (GABA\", \"choline\", \"citric acid\", \"malate\", \"asparagine\", \n",
    "#       \"quinic acid\", \"maltol\", \"nodakenetin\", \"nodakenin\", \"columbianetin\", \"decursin\", \"ferulic acid\", \"caffeic acid\",\n",
    "#       \"aegelinol\", \"cis-chlorogenic acid\", \"chlorogenic acid\", \"esculetin\"]\n",
    "\n",
    "# number=4\n",
    "# text=[\"piperine\", \"piperanine\", \"piperlonguminine\", \"retrofractamide B\", \"dehydropipernonaline\", \"Methyl piperate\", \"pipernonaline\", \"Retrofractamide A\", \"retrofractamide C\"]\n",
    "\n",
    "# number=5\n",
    "# text=[\"apocynin\", \"resacetophenone\", \"4-hydroxyacetophenone\", \"2,5-dihydroxyacetophenone\", \"physcion\"]\n",
    "\n",
    "# number=6\n",
    "# text=[\"protocatechuic acid\", \"p-hydroxybenzaldehyde\", \"choline\", \"gluconate\", \"myo-inositol\", \"thymine\", \"uracil\", \"vanillin\", \"vanillic acid\", \"protocatechuic aldehyde\", \"trans-aconitate\", \"ferulic acid\", \"caffeic acid\", \"chlorogenic acid\", \"linoleic acid\", \"neochlorogenic acid\", \"xanthatin\", \"cynarin\", \"cynarine\"]\n",
    "\n",
    "# number=7\n",
    "# text=[\"atractylenolide III\", \"vanillic acid\", \"esculetin\", \"elemicin\", \"adenosine\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "951418c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#결과 후처리\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "import pickle\n",
    "\n",
    "number=7\n",
    "\n",
    "with open(\"result/{}차/output_dict.pickle\".format(number),\"rb\") as fi:\n",
    "    output_dict = pickle.load(fi)\n",
    "    \n",
    "# validation_df = pd.read_csv('result/{}차/{}_검증.csv'.format(number,number))\n",
    "inferred_phenotype2 = pd.read_csv('result/{}차/{}_network_score.csv'.format(number,number))\n",
    "\n",
    "phenotype_rwr_score_dict = pd.Series(inferred_phenotype2.Network_score.values, index=inferred_phenotype2.Phenotype).to_dict()\n",
    "\n",
    "temp=list(phenotype_rwr_score_dict.keys())\n",
    "for i in temp:\n",
    "    if np.isnan(phenotype_rwr_score_dict[i])==True:\n",
    "        phenotype_rwr_score_dict.pop(i)\n",
    "\n",
    "Phenotype_score_Phenotype=list(phenotype_rwr_score_dict.keys())\n",
    "Phenotype_score_Score=list(phenotype_rwr_score_dict.values())\n",
    "\n",
    "temp2=list(output_dict.keys())\n",
    "for i in temp2:\n",
    "    if np.isnan(output_dict[i][0])==True:\n",
    "        output_dict.pop(i)\n",
    "        \n",
    "P_value_Phenotype=list(output_dict.keys())\n",
    "P_value_Score=list(output_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e85ef04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Name    UMLSID\n",
      "0                              Arachnodactyly  C0003706\n",
      "1                                      Stroke      None\n",
      "2                                Hypohidrosis  C0020620\n",
      "3                                Hyperkalemia  C0020461\n",
      "4     Decreased circulating aldosterone level  C0857899\n",
      "...                                       ...       ...\n",
      "6704    Abnormal full-field electroretinogram  C4072956\n",
      "6705  Abnormal fundus fluorescein angiography  C4073074\n",
      "6706    Abnormal multifocal electroretinogram  C4072958\n",
      "6707                      Mongolian blue spot      None\n",
      "6708                           Absent malleus      None\n",
      "\n",
      "[6709 rows x 2 columns]\n",
      "걸린 시간 :  0:29:21.689884\n",
      "걸린 시간 : 1761.68988 sec\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "z_score_list=[]\n",
    "outlier_list=[] #제거가능\n",
    "P_value_list=[]\n",
    "rank_number=0\n",
    "\n",
    "for i in range(len(P_value_Score)):\n",
    "    Numberlist=P_value_Score[i].copy()\n",
    "    tmp1=P_value_Phenotype[i]\n",
    "    tmp2=Phenotype_score_Phenotype.index(tmp1)\n",
    "    tmp3=Phenotype_score_Score[tmp2]\n",
    "    Numberlist.append(tmp3)\n",
    "    \n",
    "    \n",
    "    # 평균 계산\n",
    "    mean = calculate_mean(Numberlist)\n",
    "\n",
    "    # 표준편차 계산\n",
    "    std_deviation = calculate_standard_deviation(Numberlist)\n",
    "\n",
    "    z_score=(Phenotype_score_Score[i]-mean)/std_deviation\n",
    "    z_score_list.append(z_score)\n",
    "    if z_score >=-2.58 and z_score<=2.58: #제거가능\n",
    "        outlier=1\n",
    "        count=count+1\n",
    "    else:\n",
    "        outlier=0 \n",
    "    outlier_list.append(outlier) #제거가능\n",
    "\n",
    "    Numberlist.sort(reverse=True)\n",
    "\n",
    "    rank=Numberlist.index(tmp3) #순위찾기\n",
    "    if rank<5:\n",
    "        rank_number=rank_number+1\n",
    "    P_value_list.append(rank)\n",
    "\n",
    "# postgresql DB 연동\n",
    "config = configparser.ConfigParser()\n",
    "config.read('db_config.ini')\n",
    "conn=psycopg2.connect(host=\"168.131.30.66\", dbname=\"coconut\",user=\"dbuser\",password=\"jnudl1\") #데이터이름변경\n",
    "conn_cur = conn.cursor()\n",
    "#------------------------------------\n",
    "phenotype_umlsid = []\n",
    "# phenotype_list를 순회하면서 각 이름에 대한 UMLS ID를 가져옴\n",
    "for phen_name in Phenotype_score_Phenotype:\n",
    "    # 이름에 단일 따옴표가 포함된 경우 이스케이프\n",
    "#     escaped_name = phen_name.replace(\"'\", \"''\")\n",
    "    \n",
    "#     conn_cur.execute(f\"SELECT umlsid FROM phenotype WHERE name = '{escaped_name}'\")\n",
    "    conn_cur.execute(\"SELECT umlsid FROM phenotype WHERE name = %s\",(phen_name,))\n",
    "    umlsid_result = conn_cur.fetchone()\n",
    "    \n",
    "    # UMLS ID가 존재하면 리스트에 추가\n",
    "    if umlsid_result:\n",
    "        phenotype_umlsid.append(umlsid_result[0])\n",
    "    else:\n",
    "        # UMLS ID가 없는 경우에 대한 처리 (예: None 또는 다른 기본값)\n",
    "        phenotype_umlsid.append(None)\n",
    "\n",
    "# 결과를 DataFrame으로 만들어 저장\n",
    "phenotype_umlsid_df = pd.DataFrame({'Name': Phenotype_score_Phenotype, 'UMLSID': phenotype_umlsid})\n",
    "print(phenotype_umlsid_df)\n",
    "\n",
    "#------------------------\n",
    "\n",
    "Phenotype_score = pd.DataFrame({\n",
    "    'Phenotype': Phenotype_score_Phenotype,\n",
    "    'Network_score': Phenotype_score_Score,\n",
    "    'z-score': z_score_list,\n",
    "    'P-value': P_value_list,\n",
    "    'UMLSID': phenotype_umlsid\n",
    "})\n",
    "\n",
    "Phenotype_score=Phenotype_score.sort_values(by='P-value',ascending=True)\n",
    "\n",
    "#     # Compound ID에 해당하는 inferred phenotype.csv 생성\n",
    "# if not os.path.exists('compound 결과'):\n",
    "#     os.makedirs('compound 결과')\n",
    "Phenotype_score.to_csv('result/{}차/{}_검증.csv'.format(number,number),index=False, encoding='UTF-8')\n",
    "    \n",
    "end = time.time()\n",
    "sec = (end - start)\n",
    "result = datetime.timedelta(seconds=sec)\n",
    "print(\"걸린 시간 : \",result)\n",
    "print(f\"걸린 시간 : {end - start:.5f} sec\")\n",
    "\n",
    "# 엑셀파일이 'phenotype mappingv3.xlsx', '1_검증.csv' 이렇게 2개가 있어\n",
    "#  'phenotype mappingv3.xlsx' 파일은 UMLSID, Group, GroupID 이렇게 3개의 컬럼이 존재하고\n",
    "# '1_검증.csv' 파일은 Phenotype, Network_score, z-score, P-value, UMLSID 이렇게 5개의 컬럼이 존재해\n",
    "# 여기서 '1_검증.csv' 파일 내에 있는 UMLSID의 데이터와  'phenotype mappingv3.xlsx' 파일 내에 있는 UMLSID의 데이터가 일치하면 Group, GroupID 값을 가져와서 똑같이 써주고, 아니면 None 값을 써주고 싶어\n",
    "# 그래서 최종적으로 '1_검증.csv' 파일내에 Phenotype, Network_score, z-score, P-value, UMLSID, Group, GroupID 이렇게 7개의 컬럼이 존재하게 만들고 싶어\n",
    "# 이렇게 코드 짜줘\n",
    "\n",
    "\n",
    "# 'phenotype mappingv3.xlsx' 파일을 읽어옵니다.\n",
    "mapping_df = pd.read_excel('phenotype mappingv3.xlsx', engine='openpyxl')\n",
    "\n",
    "# '1_검증.csv' 파일 내의 UMLSID와 'phenotype mappingv3.xlsx' 파일 내의 UMLSID를 기반으로 병합합니다.\n",
    "merged_df = Phenotype_score.merge(mapping_df, on='UMLSID', how='left')\n",
    "\n",
    "# Group 및 GroupID가 없는 경우 None으로 채웁니다.\n",
    "merged_df['Group'].fillna('None', inplace=True)\n",
    "merged_df['GroupID'].fillna('None', inplace=True)\n",
    "\n",
    "# 'Network_score' 컬럼의 데이터를 추출합니다.\n",
    "network_scores = merged_df['Network_score']\n",
    "\n",
    "# Min-Max Scaling을 0부터 1로 설정하여 적용합니다.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_scores = scaler.fit_transform(network_scores.values.reshape(-1, 1))  # 2D 배열로 변환 후 스케일링\n",
    "\n",
    "# 스케일링된 값을 'scaling_score' 컬럼에 추가합니다.\n",
    "merged_df['scaling_score'] = scaled_scores\n",
    "\n",
    "# 'P-value'의 값을 조정합니다.\n",
    "def adjust_pvalue(value):\n",
    "    if value == 999 or value == 1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return (value+1) / 1000\n",
    "\n",
    "merged_df['P-value'] = merged_df['P-value'].apply(adjust_pvalue)\n",
    "\n",
    "cols_to_drop = [col for col in merged_df.columns if col.startswith(\"Unnamed\")]\n",
    "merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "hpo_df=pd.read_csv(\"Data/[original] HPO phenotype data.csv\")\n",
    "\n",
    "merged_df=pd.merge(merged_df, hpo_df, left_on='Phenotype', right_on='phenotype name', how='left')\n",
    "\n",
    "final_df=merged_df[['Phenotype','Network_score','z-score','P-value','UMLSID','Group','GroupID','scaling_score', 'hpo_id']]\n",
    "\n",
    "# 수정된 DataFrame을 새로운 엑셀 파일로 저장합니다.\n",
    "final_df.to_csv('result/{}차/{}_검증_with_scaling.csv'.format(number,number), index=False)  # 저장할 파일명을 지정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ef2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwr3",
   "language": "python",
   "name": "rwr3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
