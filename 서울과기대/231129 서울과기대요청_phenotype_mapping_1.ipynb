{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rwr 조화평균 완료\n",
    "\n",
    "# herb 최종\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import statistics\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from statistics import harmonic_mean\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def calculate_mean(numbers):\n",
    "    return sum(numbers) / len(numbers)\n",
    "\n",
    "def calculate_standard_deviation(numbers):\n",
    "    return statistics.stdev(numbers)\n",
    "\n",
    "def combine_lists(full_list):\n",
    "    combined = defaultdict(list)\n",
    "    \n",
    "    # 모든 리스트에 대한 처리\n",
    "    for sublist in full_list:\n",
    "        for item in sublist:\n",
    "            entrez_id, rwr_score = item\n",
    "            combined[entrez_id].append(rwr_score)\n",
    "    \n",
    "    # 조화평균 및 결과 리스트 만들기\n",
    "    result = []\n",
    "    for entrez_id, scores in combined.items():\n",
    "        if len(scores) == 1:\n",
    "            result.append([entrez_id, scores[0]])  # 값이 하나면 그대로 추가\n",
    "        else:\n",
    "            # 조화평균 계산\n",
    "            harmonic_mean_score = harmonic_mean(scores)\n",
    "            result.append([entrez_id, harmonic_mean_score])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compound_target(compoundid):\n",
    "\n",
    "    conn_cur.execute(f\"with tmp1 as (select cg.geneid from compound_gene cg where (cg.compid = '{compoundid}' and cg.relationsign in ('activate','inhibit')))\" \n",
    "                    f\"select g.entrezid from gene g,tmp1 where g.geneid=tmp1.geneid;\")\n",
    "    compound_gene1=conn_cur.fetchall()\n",
    "\n",
    "    compound_gene=set(compound_gene1)\n",
    "    compound_gene=list(compound_gene)\n",
    "    \n",
    "    # Compound-target gene List 생성\n",
    "    target_gene_list = [] \n",
    "    for i in range(len(compound_gene)):\n",
    "        interaction_gene = compound_gene[i][0]\n",
    "        if interaction_gene in entrez_list:\n",
    "            target_gene_list.append(interaction_gene)        \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # restart node에 적용할 target gene List 생성\n",
    "    target_node = [] \n",
    "    for entrez in target_gene_list:\n",
    "        node_ = node_index.get(entrez) \n",
    "        target_node.append(node_)\n",
    "\n",
    "    conn_cur.execute(f\"with tmp2 as (select cg.geneid from compound_gene cg where cg.compid = '{compoundid}' and relationsign='associate')\" \n",
    "                    f\"select g.entrezid from gene g,tmp2 where g.geneid=tmp2.geneid;\")\n",
    "\n",
    "    compound_gene2=conn_cur.fetchall()\n",
    "\n",
    "    compound_gene=set(compound_gene2)\n",
    "    compound_gene=list(compound_gene)\n",
    "\n",
    "    indirected_target_gene_list = [] \n",
    "    for i in range(len(compound_gene)):\n",
    "        interaction_gene = compound_gene[i][0]\n",
    "        if interaction_gene in entrez_list:\n",
    "            indirected_target_gene_list.append(interaction_gene)        \n",
    "        else:\n",
    "            continue      \n",
    "\n",
    "    num1=len(target_gene_list) #target gene list 갯수\n",
    "    directed_target_gene_list.append(num1)\n",
    "    num2=len(indirected_target_gene_list) #indirected target gene list 갯수 \n",
    "    indirected_target_gene_list_count.append(num2)\n",
    "    \n",
    "    #indirect nodes 정의\n",
    "    indirect_nodes= []\n",
    "    for entrez in indirected_target_gene_list:\n",
    "        node_ = node_index.get(entrez) \n",
    "        indirect_nodes.append(node_)\n",
    "\n",
    "    # PPI network RWR을 위한 initialize\n",
    "    initialized = RWR_initializing(G, seed_nodes=target_node,indirect_target_nodes=indirect_nodes, is_weighted=False) \n",
    "\n",
    "    # RWR 실행\n",
    "    rwr = RWR(initialized=initialized, prob=0.2, max_iter=100, tol=1.0e-6)\n",
    "    rwr_mapping_entrez = [[entrez_list[i], rwr[node_index.get(entrez_list[i])]] for i in range(len(entrez_list))]\n",
    "    \n",
    "    var_name1 = f'{compoundid}_num1'\n",
    "    var_name2 = f'{compoundid}_num2'\n",
    "\n",
    "    # 전역 변수 설정\n",
    "    globals()[var_name1] = num1\n",
    "    globals()[var_name2] = num2\n",
    "    \n",
    "    return rwr_mapping_entrez\n",
    "\n",
    "def rwr_compound_target(count,entrez_list):\n",
    "    \n",
    "    num1=directed_target_gene_list[count]\n",
    "    num2=indirected_target_gene_list_count[count]\n",
    "    \n",
    "    random_interaction_gene1 = random.sample(entrez_list, num1)\n",
    "    remaining_entrez_list = [gene for gene in entrez_list if gene not in random_interaction_gene1]\n",
    "    random_interaction_gene2 = random.sample(remaining_entrez_list, num2)\n",
    "    # restart node에 적용할 target gene List 생성\n",
    "    target_node = [] \n",
    "    for entrez in random_interaction_gene1:\n",
    "        node_ = node_index.get(entrez)\n",
    "        target_node.append(node_)\n",
    "\n",
    "    #indirect nodes 정의\n",
    "    indirect_nodes= []\n",
    "    for entrez in random_interaction_gene2:\n",
    "        node_ = node_index.get(entrez)\n",
    "        indirect_nodes.append(node_)\n",
    "\n",
    "    # PPI network RWR을 위한 initialize\n",
    "    initialized = RWR_initializing(G, seed_nodes=target_node,indirect_target_nodes=indirect_nodes, is_weighted=False) \n",
    "    \n",
    "    # RWR 실행\n",
    "    rwr=RWR(initialized=initialized, prob=0.2, max_iter=100, tol=1.0e-6)\n",
    "    rwr_mapping_entrez = [[entrez_list[i], rwr[node_index.get(entrez_list[i])]] for i in range(len(entrez_list))]\n",
    "    return rwr_mapping_entrez\n",
    "\n",
    "def create_ppi_network(network_data):\n",
    "    network_data = network_data.astype({'Entrez Gene Interactor A': str,\n",
    "                                        'Entrez Gene Interactor B': str})\n",
    "\n",
    "    # symbol list를 생성하고 unique 값만 가지도록 생성\n",
    "    symbolA = network_data.loc[:, 'Entrez Gene Interactor A'].to_list()\n",
    "    symbolB = network_data.loc[:, 'Entrez Gene Interactor B'].to_list()\n",
    "\n",
    "    symbol_list = symbolA + symbolB \n",
    "    symbol_list = set(symbol_list) \n",
    "    symbol_list = list(symbol_list)\n",
    "\n",
    "    # 모든 symbol에 index를 딕셔너리에서 부여\n",
    "    node_index = {}\n",
    "    for i in range(len(symbol_list)):\n",
    "        node_index[symbol_list[i]] = i\n",
    "\n",
    "    # index로 node_list 생성\n",
    "    node_list = node_index.values()\n",
    "    \n",
    "    # index로 edge_list 생성\n",
    "    edge_list = network_data[['Entrez Gene Interactor A', 'Entrez Gene Interactor B']].values.tolist()  #\n",
    "    for i in range(len(edge_list)):\n",
    "        edge_list[i][0] = node_index.get(edge_list[i][0])\n",
    "        edge_list[i][1] = node_index.get(edge_list[i][1])\n",
    "    \n",
    "    # 무방향 그래프 오브젝트 생성\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 그래프에 노드 추가\n",
    "    G.add_nodes_from(node_list)\n",
    "\n",
    "    # 그래프에 엣지 추가\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    print(f'Number of created nodes: {G.number_of_nodes()}')\n",
    "    print(f'Number of created edges: {G.number_of_edges()}')\n",
    "\n",
    "    return G, node_index\n",
    "\n",
    "def RWR_initializing(G, seed_nodes, indirect_target_nodes, is_weighted=False):\n",
    "    norm_A = np.zeros(shape=(len(G), len(G)))\n",
    "    if is_weighted:  \n",
    "        for i, neighbor_dict in G.adjacency():\n",
    "            for j, v in neighbor_dict.items():\n",
    "                norm_A[i][j] = v.get(\"weight\", 1 / len(neighbor_dict))\n",
    "    else:  \n",
    "        for i, neighbor_dict in G.adjacency():  \n",
    "            for j, v in neighbor_dict.items():  \n",
    "                norm_A[i][j] = 1 / len(neighbor_dict)\n",
    "    \n",
    "    # seed nodes와 indirect target nodes를 모두 포함한 딕셔너리 생성\n",
    "    personalization = {node: 1 for node in seed_nodes}\n",
    "    for node in indirect_target_nodes:\n",
    "        personalization[node] = 0.3  \n",
    "        \n",
    "    r_0 = np.array([personalization.get(n, 0) for n in range(len(G))]) \n",
    "    r_c = np.repeat(1 / len(G), len(G)) \n",
    "    return {\"norm_A\": norm_A, \"r_0\": r_0, \"r_c\": r_c, \"N\": len(G)}\n",
    "\n",
    "def RWR(initialized, prob=0.85, max_iter=100, tol=1.0e-6):\n",
    "    if initialized is None:\n",
    "        raise ValueError('initialized information must be required')\n",
    "\n",
    "    norm_A = initialized['norm_A']  # 인접행렬 A (정규화된)\n",
    "    r_0 = initialized['r_0']        # 시작 노드\n",
    "    r_c = initialized['r_c']        # 가중치 행렬\n",
    "    N = initialized['N']            # 노드의 개수\n",
    "    for iteration in range(max_iter):\n",
    "        r_prev = r_c\n",
    "        r_c = prob * r_c @ norm_A + (1 - prob) * r_0  # RWR algorithm\n",
    "        err = np.absolute(r_c - r_prev).sum()\n",
    "        if err < N * tol:\n",
    "#             print(f'RWR iteration = {iteration + 1}')\n",
    "#             print('Converged')\n",
    "            return r_c\n",
    "#         else:\n",
    "#             print(f'RWR iteration = {iteration + 1}, Iteration until convergence ...')\n",
    "#             print(f'{err} -> {N * tol}')\n",
    "    return \"NotConverged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69cd481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postgresql DB 연동\n",
    "config = configparser.ConfigParser()\n",
    "config.read('db_config.ini')\n",
    "conn=psycopg2.connect(host=\"168.131.30.66\", dbname=\"coconut\",user=\"dbuser\",password=\"jnudl1\") #데이터이름변경\n",
    "conn_cur = conn.cursor()\n",
    "\n",
    "# BIOGRID-Homo_sapiens Data 기반 PPI Network 구축\n",
    "network_data = pd.read_csv(\"/data/home/sss2061/식약처/Data/BIOGRID-ORGANISM-Homo_sapiens.tab3.txt\", sep=\"\\t\", encoding=\"cp949\")  \n",
    "G, node_index = create_ppi_network(network_data=network_data)  \n",
    "entrez_list = list(node_index.keys())\n",
    "entrez_list.remove('-')\n",
    "\n",
    "# 네트워크 활용 gene_phenotype 정보 생성\n",
    "gene_phenotype_file = '/data/home/sss2061/식약처보고서/서울과기대/Data/HPO_gene_phenotype.csv'\n",
    "if os.path.isfile(gene_phenotype_file): \n",
    "    gene_phenotype_df = pd.read_csv(gene_phenotype_file, encoding='UTF-8', converters={'Phenotype': ast.literal_eval}) \n",
    "else: \n",
    "    gene_phenotype_info = {}\n",
    "    for g in range(len(entrez_list)):\n",
    "        conn_cur.execute(f\"with tmp1 as (select g.geneid from gene g where g.entrezid = '{entrez_list[g]}'),\"\n",
    "                         f\"tmp2 as (select gp.phenid from gene_phenotype gp, tmp1 where gp.geneid = tmp1.geneid)\"\n",
    "                         f\"select distinct p.name from phenotype p, tmp2 where p.phenid = tmp2.phenid;\")\n",
    "        # TASS DB 기반 gene-phenotype 정보 추출\n",
    "        selected_phenotype = conn_cur.fetchall()\n",
    "        phenotypes = []\n",
    "        for i in range(len(selected_phenotype)):\n",
    "             phenotypes.append(selected_phenotype[i][0]) \n",
    "        gene_phenotype_info[entrez_list[g]] = phenotypes\n",
    "        print(f'{g + 1}/{len(entrez_list)}')\n",
    "    gene_phenotype_mapping = [[entrez_list[i], gene_phenotype_info.get(entrez_list[i])] for i in range(len(entrez_list))]\n",
    "    gene_phenotype_df = pd.DataFrame(data=gene_phenotype_mapping, columns=['Entrez ID', 'Phenotype'])\n",
    "    gene_phenotype_df.to_csv(gene_phenotype_file, index=False, encoding='UTF-8')\n",
    "\n",
    "phenotype_list = []\n",
    "for phen_list in gene_phenotype_df['Phenotype']:\n",
    "    for phen in phen_list:\n",
    "        if phen not in phenotype_list:\n",
    "            phenotype_list.append(phen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66486f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- 1차 추가\n",
    "number=1\n",
    "text=[\"adenine\", \"canavanin\", \"malate\", \"isoleucine\", \"phenylalanine\", \"tyrosine\", \"isocitrate\", \"glutamine\", \"tryptophan\",\"L-malate\", \n",
    "      \"theogallin\", \"trans-aconitate\", \"cis-aconitate\", \"threonate\", \"benzoic acid\", \"choline\", \"citric acid\", \"coumarin\", \"phloroglucinol\", \"gallic acid\", \n",
    "      \"citrullin\", \"pipecolic acid\", \"vitamin B\", \"pantothenic acid\", \"DL-valine\", \"vanillin\", \"theophylline\", \"caffeine\", \"protocatechuic aldehyde\", \"adenosine\", \n",
    "      \"EGCG\", \"gallocatechin\", \"N-acetylglutamate\", \"loliolide\", \"procyanidin B2\", \"3H-proline\", \"dihydromyricetin\", \"beta-alanine betaine\", \"epicatechin gallate (ECG\", \"5'-methylthioadenosine\", \n",
    "      \"epiafzelechin\", \"ferulic acid\", \"p-coumaric acid\", \"caffeic acid\", \"chlorogenic acid\", \"quercetin\", \"alpha-isopropylmalate\", \"neochlorogenic acid\", \"rutin\", \"hyperoside\", \n",
    "      \"4-p-coumaroylquinic acid\", \"astragalin\", \"quercetin 7-O-glucoside\", \"p-coumaroylquinic acid\", \"procyanidin B1\"]\n",
    "\n",
    "Compoundid_list=[]\n",
    "for compound in text:\n",
    "    # SQL 쿼리에서 따옴표 이스케이프\n",
    "#     compound_escaped = compound.replace(\"'\", \"''\")\n",
    "    conn_cur.execute(\"SELECT compid FROM compound WHERE name = %s\", (compound,))\n",
    "    result = conn_cur.fetchone()\n",
    "    if result:\n",
    "        Compoundid_list.append(result[0])\n",
    "    else:\n",
    "        Compoundid_list.append(None)  # 데이터가 없는 경우 None 추가\n",
    "\n",
    "if not os.path.exists('result/{}차'.format(number)):\n",
    "    os.makedirs('result/{}차'.format(number)) \n",
    "    \n",
    "Compound_list=copy.deepcopy(Compoundid_list)\n",
    "    \n",
    "#------------- 1차 끝\n",
    "\n",
    "global directed_target_gene_list\n",
    "directed_target_gene_list=[]  #사용자가 입력한 compound의 direct target gene 개수가 들어있는 list\n",
    "\n",
    "global indirected_target_gene_list_count\n",
    "indirected_target_gene_list_count=[] #사용자가 입력한 compound의 indirect target gene 개수가 들어있는 list\n",
    "\n",
    "rwr_mapping_entrez_list=[] #사용자가 입력한 compound 별 rwr mapping entrez가 들어있는 list\n",
    "\n",
    "for c in range(len(Compound_list)):\n",
    "    \n",
    "    compoundid1=Compound_list[c]\n",
    "    \n",
    "    rwr_mapping_entrez=compound_target(compoundid1)\n",
    "    rwr_mapping_entrez_list.append(rwr_mapping_entrez)\n",
    "\n",
    "rwr_mapping_entrez2 = combine_lists(rwr_mapping_entrez_list) #rwr mapping 조화평균 추출\n",
    "\n",
    "rwr_result = pd.DataFrame(data=rwr_mapping_entrez2, columns=['Entrez ID', 'RWR_score'])\n",
    "rwr_result['Entrez ID'] = rwr_result['Entrez ID'].astype(int)\n",
    "rwr_result = pd.merge(rwr_result, gene_phenotype_df, how='outer')\n",
    "rwr_result['Phenotype'].loc[rwr_result['Phenotype'].isnull()] = rwr_result['Phenotype'].loc[\n",
    "    rwr_result['Phenotype'].isnull()].apply(lambda x: [])\n",
    "rwr_result = rwr_result.sort_values(by='RWR_score', ascending=False)\n",
    "\n",
    "# phenotype 별 RWR score를 계산하기 위한 딕셔너리 생성\n",
    "phenotype_rwr_score_dict = {}\n",
    "for phen in phenotype_list:\n",
    "    phenotype_rwr_score_dict[phen] = 0    \n",
    "    \n",
    "# 각 phenotype 별 RWR score 확인\n",
    "for i in range(len(rwr_result)):\n",
    "    rwr_score = rwr_result['RWR_score'].iloc[i]\n",
    "    if rwr_result['Phenotype'].iloc[i]:\n",
    "        for phen in rwr_result['Phenotype'].iloc[i]:\n",
    "            phenotype_rwr_score_dict[phen] += rwr_score\n",
    "\n",
    "temp=list(phenotype_rwr_score_dict.keys())\n",
    "for i in temp:\n",
    "    if np.isnan(phenotype_rwr_score_dict[i])==True:\n",
    "        phenotype_rwr_score_dict.pop(i)\n",
    "        \n",
    "Phenotype_score_Phenotype=list(phenotype_rwr_score_dict.keys())\n",
    "Phenotype_score_Score=list(phenotype_rwr_score_dict.values())\n",
    "\n",
    "#network score. csv 파일로 저장\n",
    "inferred_phenotype2 = pd.DataFrame(data=[pair for pair in zip(list(phenotype_rwr_score_dict.keys()),\n",
    "                                                             list(phenotype_rwr_score_dict.values()))],\n",
    "                                  columns=['Phenotype', 'Network_score'])\n",
    "inferred_phenotype2 = inferred_phenotype2.sort_values(by='Network_score', ascending=False)\n",
    "inferred_phenotype2.to_csv('result/{}차/{}_network_score.csv'.format(number,number),index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fe079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"result/{}차/output_dict.pickle\".format(number),\"rb\") as fi:\n",
    "#     output_dict = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------1,000\n",
    "for h in tqdm_notebook(range(1000)):\n",
    "    p_rwr_mapping_entrez_list=[] #1번당 compound별 random target gene rwr mapping list\n",
    "    \n",
    "    print(h,\"번째 상황\") # 제거\n",
    "    for k in tqdm_notebook(range(len(Compound_list))):\n",
    "        rwr_mapping_entrez=rwr_compound_target(k,entrez_list)\n",
    "        p_rwr_mapping_entrez_list.append(rwr_mapping_entrez)\n",
    "    \n",
    "    rwr_mapping_entrez3 = combine_lists(p_rwr_mapping_entrez_list) #rwr mapping 조화평균 추출\n",
    "    \n",
    "    rwr_result = pd.DataFrame(data=rwr_mapping_entrez3, columns=['Entrez ID', 'RWR_score'])\n",
    "    rwr_result['Entrez ID'] = rwr_result['Entrez ID'].astype(int)\n",
    "    rwr_result = pd.merge(rwr_result, gene_phenotype_df, how='outer')\n",
    "    rwr_result['Phenotype'].loc[rwr_result['Phenotype'].isnull()] = rwr_result['Phenotype'].loc[rwr_result['Phenotype'].isnull()].apply(lambda x: [])\n",
    "    rwr_result = rwr_result.sort_values(by='RWR_score', ascending=False)\n",
    "    \n",
    "# phenotype 별 RWR score를 계산하기 위한 딕셔너리 생성\n",
    "    phenotype_rwr_score_dict = {}\n",
    "    for phen in phenotype_list:\n",
    "        phenotype_rwr_score_dict[phen] = 0\n",
    "\n",
    "# 각 phenotype 별 RWR score 확인\n",
    "    for i in range(len(rwr_result)):\n",
    "        rwr_score = rwr_result['RWR_score'].iloc[i]\n",
    "        if rwr_result['Phenotype'].iloc[i]:\n",
    "            for phen in rwr_result['Phenotype'].iloc[i]:\n",
    "                phenotype_rwr_score_dict[phen] += rwr_score\n",
    "\n",
    "    key_list=list(phenotype_rwr_score_dict.keys())\n",
    "    for i in key_list:\n",
    "        if i not in output_dict:\n",
    "            output_dict[i]=list([phenotype_rwr_score_dict[i]])\n",
    "        else:\n",
    "            output_dict[i].append(phenotype_rwr_score_dict[i])\n",
    "    \n",
    "    with open(\"result/{}차/output_dict.pickle\".format(number),\"wb\") as f:\n",
    "        pickle.dump(output_dict,f)\n",
    "    \n",
    "print(\"완료1\")\n",
    "\n",
    "temp2=list(output_dict.keys())\n",
    "for i in temp2:\n",
    "    if np.isnan(output_dict[i][0])==True:\n",
    "        output_dict.pop(i)\n",
    "        \n",
    "#rwr. score csv 파일로 저장\n",
    "inferred_phenotype3 = pd.DataFrame(data=[pair for pair in zip(list(output_dict.keys()),\n",
    "                                                             list(output_dict.values()))],\n",
    "                                  columns=['Phenotype', 'Network_score'])\n",
    "inferred_phenotype3 = inferred_phenotype3.sort_values(by='Network_score', ascending=False)\n",
    "\n",
    "inferred_phenotype3.to_csv('result/{}차/{}_P_value.csv'.format(number,number),index=False, encoding='UTF-8')\n",
    "\n",
    "P_value_Phenotype=list(output_dict.keys())\n",
    "P_value_Score=list(output_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output_dict['Autosomal recessive inheritance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------z-score  \n",
    "count=0\n",
    "z_score_list=[]\n",
    "outlier_list=[] #제거가능\n",
    "P_value_list=[]\n",
    "rank_number=0\n",
    "result_phenotype=[]\n",
    "result_phenotype_score=[]\n",
    "\n",
    "for i in range(len(P_value_Score)):\n",
    "    Numberlist=P_value_Score[i].copy()\n",
    "    tmp1=P_value_Phenotype[i]\n",
    "    tmp2=Phenotype_score_Phenotype.index(tmp1)\n",
    "    tmp3=Phenotype_score_Score[tmp2]\n",
    "    Numberlist.append(tmp3)\n",
    "    result_phenotype.append(tmp1)\n",
    "    result_phenotype_score.append(tmp3)\n",
    "    \n",
    "    # 평균 계산\n",
    "    mean = calculate_mean(Numberlist)\n",
    "    # 표준편차 계산\n",
    "    std_deviation = calculate_standard_deviation(Numberlist)\n",
    "\n",
    "    z_score=(Phenotype_score_Score[i]-mean)/std_deviation\n",
    "    z_score_list.append(z_score)\n",
    "    if z_score >=-2.58 and z_score<=2.58: #제거가능\n",
    "        outlier=1\n",
    "        count=count+1\n",
    "    else:\n",
    "        outlier=0 \n",
    "    outlier_list.append(outlier) #제거가능\n",
    "\n",
    "    Numberlist.sort(reverse=True)\n",
    "\n",
    "    rank=Numberlist.index(tmp3) #순위찾기\n",
    "    if rank<5:\n",
    "        rank_number=rank_number+1\n",
    "    P_value_list.append(rank)\n",
    "\n",
    "#------------------------\n",
    "\n",
    "Phenotype_score = pd.DataFrame({\n",
    "    'Phenotype': result_phenotype,\n",
    "    'Network_score': result_phenotype_score,\n",
    "    'z-score': z_score_list,\n",
    "    'P-value': P_value_list,\n",
    "})\n",
    "\n",
    "Phenotype_score=Phenotype_score.sort_values(by='P-value',ascending=True)\n",
    "\n",
    "#------------------------------------\n",
    "#umls db에서 umls id와 phenotype score 파일을 합치는 코드\n",
    "\n",
    "file_2= pd.read_csv(\"Data/수정_Id2.csv\")\n",
    "\n",
    "# file과 file_2를 'Phenotype'과 'phenotype' 컬럼을 기준으로 결합합니다.\n",
    "# 이때 file_2의 'umlsid' 컬럼만 선택하여 추가합니다.\n",
    "file_3 = pd.merge(Phenotype_score, file_2[['umlsid', 'phenotype']], left_on='Phenotype', right_on='phenotype', how='left')\n",
    "\n",
    "# 중복된 'phenotype' 컬럼을 제거합니다.\n",
    "file_3.drop(columns=['phenotype'], inplace=True)\n",
    "Phenotype_score=file_3\n",
    "#------------------------------------\n",
    "\n",
    "#     # Compound ID에 해당하는 inferred phenotype.csv 생성\n",
    "# if not os.path.exists('compound 결과'):\n",
    "#     os.makedirs('compound 결과')\n",
    "Phenotype_score.to_csv('result/{}차/{}_검증.csv'.format(number,number),index=False, encoding='UTF-8')\n",
    "    \n",
    "end = time.time()\n",
    "sec = (end - start)\n",
    "result = datetime.timedelta(seconds=sec)\n",
    "print(\"걸린 시간 : \",result)\n",
    "print(f\"걸린 시간 : {end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀파일이 'phenotype mappingv3.xlsx', '1_검증.csv' 이렇게 2개가 있어\n",
    "#  'phenotype mappingv3.xlsx' 파일은 UMLSID, Group, GroupID 이렇게 3개의 컬럼이 존재하고\n",
    "# '1_검증.csv' 파일은 Phenotype, Network_score, z-score, P-value, UMLSID 이렇게 5개의 컬럼이 존재해\n",
    "# 여기서 '1_검증.csv' 파일 내에 있는 UMLSID의 데이터와  'phenotype mappingv3.xlsx' 파일 내에 있는 UMLSID의 데이터가 일치하면 Group, GroupID 값을 가져와서 똑같이 써주고, 아니면 None 값을 써주고 싶어\n",
    "# 그래서 최종적으로 '1_검증.csv' 파일내에 Phenotype, Network_score, z-score, P-value, UMLSID, Group, GroupID 이렇게 7개의 컬럼이 존재하게 만들고 싶어\n",
    "# 이렇게 코드 짜줘\n",
    "# group id 기준으로 그룹핑 매핑\n",
    "\n",
    "# 'phenotype mappingv3.xlsx' 파일을 읽어옵니다.\n",
    "mapping_df = pd.read_excel('phenotype mappingv3.xlsx', engine='openpyxl')\n",
    "\n",
    "# '1_검증.csv' 파일 내의 UMLSID와 'phenotype mappingv3.xlsx' 파일 내의 UMLSID를 기반으로 병합합니다.\n",
    "merged_df = Phenotype_score.merge(mapping_df, on='UMLSID', how='left')\n",
    "\n",
    "# Group 및 GroupID가 없는 경우 None으로 채웁니다.\n",
    "merged_df['Group'].fillna('None', inplace=True)\n",
    "merged_df['GroupID'].fillna('None', inplace=True)\n",
    "\n",
    "# 'Network_score' 컬럼의 데이터를 추출합니다.\n",
    "network_scores = merged_df['Network_score']\n",
    "\n",
    "# Min-Max Scaling을 0부터 1로 설정하여 적용합니다.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_scores = scaler.fit_transform(network_scores.values.reshape(-1, 1))  # 2D 배열로 변환 후 스케일링\n",
    "\n",
    "# 스케일링된 값을 'scaling_score' 컬럼에 추가합니다.\n",
    "merged_df['scaling_score'] = scaled_scores\n",
    "\n",
    "# 'P-value'의 값을 조정합니다.\n",
    "def adjust_pvalue(value):\n",
    "    if value == 999 or value == 1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return (value+1) / 1000\n",
    "\n",
    "merged_df['P-value'] = merged_df['P-value'].apply(adjust_pvalue)\n",
    "\n",
    "cols_to_drop = [col for col in merged_df.columns if col.startswith(\"Unnamed\")]\n",
    "merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "hpo_df=pd.read_csv(\"Data/[original] HPO phenotype data.csv\")\n",
    "\n",
    "merged_df=pd.merge(merged_df, hpo_df, left_on='Phenotype', right_on='phenotype name', how='left')\n",
    "\n",
    "final_df=merged_df[['Phenotype','Network_score','z-score','P-value','UMLSID','Group','GroupID','scaling_score', 'hpo_id']]\n",
    "\n",
    "# 수정된 DataFrame을 새로운 엑셀 파일로 저장합니다.\n",
    "final_df.to_csv('result/{}차/{}_검증_with_scaling.csv'.format(number,number), index=False)  # 저장할 파일명을 지정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number=1\n",
    "# text=[\"adenine\", \"canavanin\", \"malate\", \"isoleucine\", \"phenylalanine\", \"tyrosine\", \"isocitrate\", \"glutamine\", \"tryptophan\",\"L-malate\", \n",
    "#       \"theogallin\", \"trans-aconitate\", \"cis-aconitate\", \"threonate\", \"benzoic acid\", \"choline\", \"citric acid\", \"coumarin\", \"phloroglucinol\", \"gallic acid\", \n",
    "#       \"citrullin\", \"pipecolic acid\", \"vitamin B\", \"pantothenic acid\", \"DL-valine\", \"vanillin\", \"theophylline\", \"caffeine\", \"protocatechuic aldehyde\", \"adenosine\", \n",
    "#       \"EGCG\", \"gallocatechin\", \"N-acetylglutamate\", \"loliolide\", \"procyanidin B2\", \"3H-proline\", \"dihydromyricetin\", \"beta-alanine betaine\", \"epicatechin gallate (ECG\", \"5'-methylthioadenosine\", \n",
    "#       \"epiafzelechin\", \"ferulic acid\", \"p-coumaric acid\", \"caffeic acid\", \"chlorogenic acid\", \"quercetin\", \"alpha-isopropylmalate\", \"neochlorogenic acid\", \"rutin\", \"hyperoside\", \n",
    "#       \"4-p-coumaroylquinic acid\", \"astragalin\", \"quercetin 7-O-glucoside\", \"p-coumaroylquinic acid\", \"procyanidin B1\"]\n",
    "\n",
    "# number=2\n",
    "# text=[\"gamma-aminobutyric acid (GABA\", \"canavanin\", \"sucrose\", \"folate\", \"guanosine\", \"vanillic acid\", \"ononin\", \n",
    "#       \"formononetin\", \"calycosin\", \"sissotrin\", \"afrormosin\", \"pratensein\"]\n",
    "\n",
    "# number=3\n",
    "# text=[\"protocatechuic acid\", \"gamma-aminobutyric acid (GABA\", \"choline\", \"citric acid\", \"malate\", \"asparagine\", \n",
    "#       \"quinic acid\", \"maltol\", \"nodakenetin\", \"nodakenin\", \"columbianetin\", \"decursin\", \"ferulic acid\", \"caffeic acid\",\n",
    "#       \"aegelinol\", \"cis-chlorogenic acid\", \"chlorogenic acid\", \"esculetin\"]\n",
    "\n",
    "# number=4\n",
    "# text=[\"piperine\", \"piperanine\", \"piperlonguminine\", \"retrofractamide B\", \"dehydropipernonaline\", \"Methyl piperate\", \"pipernonaline\", \"Retrofractamide A\", \"retrofractamide C\"]\n",
    "\n",
    "# number=5\n",
    "# text=[\"apocynin\", \"resacetophenone\", \"4-hydroxyacetophenone\", \"2,5-dihydroxyacetophenone\", \"physcion\"]\n",
    "\n",
    "# number=6\n",
    "# text=[\"protocatechuic acid\", \"p-hydroxybenzaldehyde\", \"choline\", \"gluconate\", \"myo-inositol\", \"thymine\", \"uracil\", \"vanillin\", \"vanillic acid\", \"protocatechuic aldehyde\", \"trans-aconitate\", \"ferulic acid\", \"caffeic acid\", \"chlorogenic acid\", \"linoleic acid\", \"neochlorogenic acid\", \"xanthatin\", \"cynarin\", \"cynarine\"]\n",
    "\n",
    "# number=7\n",
    "# text=[\"atractylenolide III\", \"vanillic acid\", \"esculetin\", \"elemicin\", \"adenosine\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f85e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene의 연관성을 homo sapiens dataset에서 찾으시는거라면 큰 문제가 없을 것 같지만, compound가 homo sapiens에게 영향을 주는 gene만을 찾으시려면 ncbitaxid가 9606인 gene만 입력되도록 코드를 추가하시는것도 좋을 것 같습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwr",
   "language": "python",
   "name": "rwr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
